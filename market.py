import streamlit as st
import cx_Oracle
import pandas as pd
from llama_cpp import Llama
import re
import numpy as np
import warnings
from io import BytesIO
from sklearn.model_selection import StratifiedGroupKFold
from scipy.stats import ks_2samp
from sklearn.model_selection import train_test_split
import random
from datetime import datetime
from sqlalchemy import text
from Jira import jira_main
from itertools import islice
import logging
from SQL_helper import sql_main
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.base import MIMEBase
from email import encoders
import os.path
import getpass
from campaign_responsible import main_checker
warnings.filterwarnings('ignore')
import plotly.express as px
from typing import Optional, Dict, Tuple
from functools import lru_cache
import json
import requests
import plotly.graph_objects as go
import os
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_community.llms import Ollama
import webbrowser
from streamlit_extras.colored_header import colored_header
from streamlit_extras.add_vertical_space import add_vertical_space
from streamlit_extras.card import card
from streamlit_extras.metric_cards import style_metric_cards
from streamlit_extras.switch_page_button import switch_page
from streamlit_extras.stateful_button import button
from streamlit_extras.tags import tagger_component
import io
import pyarrow as pa
import pyarrow.parquet as pq

def run_main_app():

    now = datetime.now()
   
    # Database connection function
    def get_connection():
        dsn = cx_Oracle.makedsn('', '', sid='')
        return cx_Oracle.connect(user='', password='', dsn=dsn)

    def get_connection_DSSB_OCDS():
        dsn = cx_Oracle.makedsn('', '', sid='')
        return cx_Oracle.connect(user='', password='', dsn=dsn)

    def get_connection_SPSS_OCDS():
        dsn = cx_Oracle.makedsn('', '', sid='')
        return cx_Oracle.connect(user='', password='', dsn=dsn)

    def get_connection_ED_OCDS():
        dsn = cx_Oracle.makedsn('', '', sid='')
        return cx_Oracle.connect(user='', password='', dsn=dsn)

    def clear_extra_iin():
        with get_connection_SPSS_OCDS() as conn:
            today = datetime.today()
            today_str = today.strftime("%d.%m.%Y")
            sql_query = f"""
                select f.p_sid, 1 as spssfl
                from spss_ocds.fd_rb2_campaigns_users f
                where f.upload_date = to_date('{today_str}','dd.mm.yyyy')
                group by f.p_sid
                having count(*)>1
                """
            constraint_df1 = pd.read_sql(sql_query, conn)
           
            st.session_state.df = st.session_state.df.merge(constraint_df1, on='P_SID', how='left')

    def fetch_campaign_data_by_id(conn, table_name):
        # Assuming 'campaign_id' is the column name in your table that holds the campaign ID.

        query = f"select max(CAMPAIGNCODE) as CAMPAIGNCODE from {table_name} where length(CAMPAIGNCODE) = 10 and CAMPAIGNCODE like 'C0000%'"

        cursor = conn.cursor()
        cursor.execute(query)
        result = cursor.fetchone()
       
        if result:
            # Assuming the result is a tuple in the order of the SQL table columns
            # Convert this to a dictionary where keys are the column names for easier access
            columns = [desc[0] for desc in cursor.description]
            return dict(zip(columns, result))
        else:
            return None

    def fetch_campaign_data_by_id_main(conn, CAMPAIGNCODE, table_name):

        query = f"SELECT * FROM {table_name} WHERE CAMPAIGNCODE = :CAMPAIGNCODE"
   
        cursor = conn.cursor()
        cursor.execute(query, {'CAMPAIGNCODE': CAMPAIGNCODE})
        result = cursor.fetchone()
       
        if result:
            columns = [desc[0] for desc in cursor.description]
            return dict(zip(columns, result))
        else:
            return None

    def fetch_campaign_data_by_XLS(conn):
        # Assuming 'campaign_id' is the column name in your table that holds the campaign ID.

        query = f"select max(XLS_OW_ID) as XLS_OW_ID from dssb_ocds.rb3_tr_campaign_dict where length(XLS_OW_ID) = 8 and XLS_OW_ID like 'KKB_%'"

        cursor = conn.cursor()
        cursor.execute(query)
        result = cursor.fetchone()
       
        if result:
            # Assuming the result is a tuple in the order of the SQL table columns
            # Convert this to a dictionary where keys are the column names for easier access
            columns = [desc[0] for desc in cursor.description]
            return dict(zip(columns, result))
        else:
            return None

    def send_email(email_recipient, email_subject, email_message, attachment_location = ''):
   
            email_sender = 'cdsrb@halykbank.kz'
   
            msg = MIMEMultipart()
            msg['From'] = email_sender
            msg['To'] = ",".join(email_recipient)
            msg['Subject'] = email_subject
   
            msg.attach(MIMEText(email_message, 'plain'))
            if attachment_location != '':  
                filename = os.path.basename(attachment_location)
                attachment = open(attachment_location, "rb")
                part = MIMEBase('application', 'octet-stream')
                part.set_payload(attachment.read())
                encoders.encode_base64(part)      
                part.add_header('Content-Disposition',"attachment; filename= %s" % filename)
                msg.attach(part)
   
            try:
                server = smtplib.SMTP('mail.halykbank.nb', 587)
                server.ehlo()
                server.starttls()
                server.login('cdsrb', 'oREgNwtWr9B5F4dsGjjZ')
                text = msg.as_string()
                server.sendmail(email_sender, email_recipient, text)
                print("accepted")
                print('email sent')
                server.quit()
            except:
                print("SMPT server connection error")
            return True
   
    # Query database and return DataFrame
    def query_database(selected_columns, additional_filters, user_limit):
        where_clauses = []
        for col, filter_details in additional_filters.items():
            if filter_details[0] == 'BETWEEN':
                where_clauses.append(f"{col} BETWEEN {filter_details[1]} AND {filter_details[2]}")
            elif filter_details[0] == 'IN':
                formatted_values = ', '.join([f"'{val}'" for val in filter_details[1]])
                where_clauses.append(f"{col} IN ({formatted_values})")
               
        where_clause = " AND ".join(where_clauses) if where_clauses else "1=1"
       
        query = f"SELECT {', '.join(selected_columns)} FROM dssb_app.rb_feature_store WHERE {where_clause} and SNAPSHOT_DATE = (select MAX(SNAPSHOT_DATE) from dssb_app.rb_feature_store) OFFSET 0 ROWS FETCH NEXT {user_limit} ROWS ONLY"

       
        # Connect to database and execute query
        with get_connection() as conn:
            df = pd.read_sql(query, conn)
   
        return df




    def parse_date(date_str):
        try:
            return datetime.strptime(date_str, "%d.%m.%Y")
        except ValueError:
            try:
                return datetime.strptime(date_str, "%Y-%m-%d")
            except ValueError:
                return None
   
    def calculate_age(row):
        today = datetime.today()
       
        if row is None or pd.isnull(row['AGE']):
            birth_date = None
            if row is not None and row['BIRTH_DATE'] is not None and not pd.isnull(row['BIRTH_DATE']):
                birth_date = parse_date(row['BIRTH_DATE'])
            elif row is not None and row['IIN_BIN'] is not None and len(row['IIN_BIN']) >= 6:
                birth_date_str = row['IIN_BIN'][:6]
                try:
                    birth_date = datetime.strptime(birth_date_str, "%y%m%d")
                    # Adjust for century
                    if birth_date > today:
                        birth_date = birth_date.replace(year=birth_date.year - 100)
                except ValueError:
                    return None
           
            if birth_date is None:
                return None
           
            return today.year - birth_date.year - ((today.month, today.day) < (birth_date.month, birth_date.day))
        else:
            return row['AGE']
   
    def determine_sex_by_iin(iin):
        if len(iin) > 6:
            if iin[6] in '135':
                return 'M'
            elif iin[6] in '246':
                return 'F'
        return None

    def load_sql_table(conn):
        query = 'select IIN_BIN, BIRTH_DATE, AGE, SEX_CODE  from dssb_dm.rb_clients'
        cursor = conn.cursor()
        cursor.execute(query)
        rows = cursor.fetchall()
        columns = [desc[0] for desc in cursor.description]
        df2 = pd.DataFrame(rows, columns=columns)
        return df2


    def load_sql_table_stratification(conn):
        query = 'select IIN_BIN, AGE, SEX_CODE, MARITAL_STATUS, POSITION, EDUCATION from dssb_dm.rb_clients'
        cursor = conn.cursor()
        cursor.execute(query)
        rows = cursor.fetchall()
        columns = [desc[0] for desc in cursor.description]
        df3 = pd.DataFrame(rows, columns=columns)
        return df3

    def get_iin_bin_for_filials(conn, selected_filials):
            cursor = conn.cursor()
   
            placeholders = ','.join(':' + str(i+1) for i in range(len(selected_filials)))
            query = f"""
                SELECT DISTINCT IIN_BIN AS IIN
                FROM dssb_dm.rb_clients
                WHERE FILIAL IN ({placeholders})
            """
   
            cursor.execute(query, selected_filials)
   
            results = cursor.fetchall()
   
            df_filials = pd.DataFrame(results, columns=['IIN'])
   
            return df_filials



    def filter_dataframe_extra(df2, min_age, max_age, sex_selection):
        df2['AGE'] = df2.apply(calculate_age, axis=1)
       
        if min_age is not None:
            df2 = df2[df2['AGE'] >= min_age]
        if max_age is not None:
            df2 = df2[df2['AGE'] <= max_age]
        if sex_selection:
            df2['SEX_CODE'] = df2.apply(lambda row: determine_sex_by_iin(row['IIN_BIN']) if pd.isnull(row['SEX_CODE']) else row['SEX_CODE'], axis=1)
            if 'M' in sex_selection and 'F' not in sex_selection:
                df2 = df2[df2['SEX_CODE'] == 'M']
            elif 'F' in sex_selection and 'M' not in sex_selection:
                df2 = df2[df2['SEX_CODE'] == 'F']
        return df2
   
    def filter_df1_based_on_df2(df1, df2):
        # Create a set of IIN_BIN values present in df2
        iin_bin_set = set(df2['IIN_BIN'])
   
        # Filter df1 to keep only rows where IIN is in the set of IIN_BIN values
        filtered_df1 = df1[df1['IIN'].isin(iin_bin_set)]
   
        return filtered_df1




   
   
    # Query database and return DataFrame
    def query_database_all(selected_columns, additional_filters):
        where_clauses = []
        for col, filter_details in additional_filters.items():
            if filter_details[0] == 'BETWEEN':
                where_clauses.append(f"{col} BETWEEN {filter_details[1]} AND {filter_details[2]}")
            elif filter_details[0] == 'IN':
                formatted_values = ', '.join([f"'{val}'" for val in filter_details[1]])
                where_clauses.append(f"{col} IN ({formatted_values})")
               
        where_clause = " AND ".join(where_clauses) if where_clauses else "1=1"
       
        query = f"SELECT {', '.join(selected_columns)} FROM dssb_app.rb_feature_store WHERE {where_clause} and SNAPSHOT_DATE = (select MAX(SNAPSHOT_DATE) from dssb_app.rb_feature_store)"

       
        # Connect to database and execute query
        with get_connection() as conn:
            df = pd.read_sql(query, conn)
   
        return df


    # Query database and return DataFrame
    def get_psid():      
        query = f"SELECT IIN, P_SID FROM dssb_app.rb_feature_store where SNAPSHOT_DATE = (select MAX(SNAPSHOT_DATE) from dssb_app.rb_feature_store)"
        # Connect to database and execute query
        with get_connection() as conn:
            df = pd.read_sql(query, conn)
   
        return df
   
   
   
   
    def fetch_IIN_to_keep(connection, IIN_list, status_columns):
        # Function to fetch IINs to keep, adjusted for your SQL logic
        status_checks = ' OR '.join([f"{col} = 0" for col in status_columns])
        formatted_IIN_list = ','.join(f"'{item}'" for item in IIN_list)
        sql_query = f"""
        SELECT IIN
        FROM dssb_app.rb_feature_store
        WHERE (IIN IN ({formatted_IIN_list})) AND ({status_checks})
        """
        cursor = connection.cursor()
        cursor.execute(sql_query)
        results = cursor.fetchall()
        IINs_to_keep = [result[0] for result in results]
        return IINs_to_keep
   
    def fetch_IIN_to_keep_reverce(connection, IIN_list, status_columns):
        # Function to fetch IINs to keep, adjusted for your SQL logic
        status_checks = ' OR '.join([f"{col} = 1" for col in status_columns])
        formatted_IIN_list = ','.join(f"'{item}'" for item in IIN_list)
        sql_query = f"""
        SELECT IIN
        FROM dssb_app.rb_feature_store
        WHERE (IIN IN ({formatted_IIN_list})) AND ({status_checks})
        """
        cursor = connection.cursor()
        cursor.execute(sql_query)
        results = cursor.fetchall()
        IINs_to_keep = [result[0] for result in results]
        return IINs_to_keep
   
   
    def add_columns_and_filter_nulls(df, conn, additional_table, join_column, additional_columns):
        # Construct a query to retrieve the additional columns
        # Ensure the query excludes rows where either of the additional columns is NULL
        additional_columns_str = ', '.join(additional_columns)
        query = f"""
        SELECT a.{join_column}, {additional_columns_str}
        FROM {additional_table} a
        JOIN dssb_app.rb_feature_store b ON a.{join_column} = b.{join_column}
        WHERE a.{additional_columns[0]} IS NOT NULL AND a.{additional_columns[1]} IS NOT NULL
        """
       
        # Execute the query
        additional_df = pd.read_sql(query, conn)
       
        # Merge the additional data with the original DataFrame
        final_df = pd.merge(df, additional_df, on=join_column, how='inner')
       
        return final_df
   
   
   
    # Assuming 'information_columns' and 'blacklist_columns' are provided as lists
    information_columns = ['SNAPSHOT_DATE','IIN','P_SID','PUBLIC_ID', 'IS_MAU']  # columns
   
    sum_columns = [
        'CARD_PURCHASE_FREQUENCY', 'CARD_PURCHASE_SUM', 'LOAN_CNT',    'SAFE_CNT',    'ACCOUNT_CNT', 'CREDIT_CARD_CNT', 'DEBIT_CARD_CNT', 'BEZNAL_TRANSACTION_CNT' ,'BEZNAL_AMOUNT', 'OBNAL_TRANSACTION_CNT']
   
    stop_list_columns = [
    'CORP_CARD'
    ,'CUR_CAMP_FIZ_IN'
    ]  # blacklist columns
   
    blacklist_columns = [
    'BL_BANKRUPT'
    ,'BL_DECEASED'
    ]
   
    ALL_STOP_LIST = [
    'EPG_ECG_FL'
    ]
   
    Extra_STOP_LIST = ['SALARY_FL']
   
   
    def fetch_IINs_to_exclude(conn, selected_stop_list_columns, user_id_col_name, user_ids, user_limit):
        """
        Fetch IINs that should be excluded based on stop list and blacklist columns.
        """
        # Construct SQL query to select IINs that match stop list or blacklist criteria
        status_checks = ' OR '.join([f"{col} = 1" for col in selected_stop_list_columns if col.strip()])
        if not status_checks:
            return []
   
        user_ids_str = ','.join(f"'{item}'" for item in user_ids)
   
        sql_query = f"""
        SELECT IIN
        FROM dssb_app.rb_feature_store
        WHERE {status_checks} AND {user_id_col_name} IN ({user_ids_str})
        OFFSET 0 ROWS FETCH NEXT {user_limit} ROWS ONLY
        """
   
   
        cursor = conn.cursor()
        cursor.execute(sql_query)
        results = cursor.fetchall()
        cursor.close()
   
        # Extract IINs from query results
        excluded_IINs = [result[0] for result in results]
       
        return excluded_IINs

    #def fetch_data(table_name, conn):
        #"""Fetch user IDs from a specific table in the Oracle database."""
        #query = f"SELECT DISTINCT IIN FROM {table_name}"
        #return pd.read_sql(query, conn)

    tables = ['ACRM_DW.RB_BLACK_LIST@ACRM', 'dssb_de.dim_clients_black_list', 'SPSS_USER_DRACRM.HALYK_JOB@SPSS_LNK', 'SPSS_USER_DRACRM.BLOGGERS@SPSS_LNK' ,  'dssb_app.not_recommend_credits']

    def fetch_data(table):

        if table == 'ACRM_DW.RB_BLACK_LIST@ACRM':
            parquet_file = 'Databases/ACRM_DW.RB_BLACK_LIST@ACRM.parquet'
        if table == 'dssb_de.dim_clients_black_list':
            parquet_file = 'Databases/dssb_de.dim_clients_black_list.parquet'
        if table == 'SPSS_USER_DRACRM.HALYK_JOB@SPSS_LNK':
            parquet_file = 'Databases/SPSS_USER_DRACRM.HALYK_JOB@SPSS_LNK.parquet'
        if table == 'SPSS_USER_DRACRM.BLOGGERS@SPSS_LNK':
            parquet_file = 'Databases/SPSS_USER_DRACRM.BLOGGERS@SPSS_LNK.parquet'
        if table == 'dssb_app.not_recommend_credits':
            parquet_file = 'Databases/dssb_app.not_recommend_credits.parquet'
        if table == 'DSSB_OCDS.mb11_global_control':
            parquet_file = 'Databases/DSSB_OCDS.mb11_global_control.parquet'
        if table == 'BL_No_worker':
            parquet_file = 'Databases/BL_No_worker.parquet'
        if table == 'dssb_app.abc_nbo_only':
            parquet_file = 'Databases/dssb_app.abc_nbo_only.parquet'
        if table == 'dssb_app.abc_ptb_models':
            parquet_file = 'Databases/dssb_app.abc_ptb_models.parquet'
        if table == 'dssb_app.abc_nbo_and_market':
            parquet_file = 'Databases/dssb_app.abc_nbo_and_market.parquet'
        return pd.read_parquet(parquet_file)

    #def fetch_data_push(table_name, conn):
    #    """Fetch user IDs from a specific table in the Oracle database."""
    #    query = f"select distinct g.iin from DSSB_SE.UCS_HB_PUSH po left join dssb_se.gid_map2 g on po.owid = g.p_sid_ow group by g.iin having #min(po.is_push_off) = 1"
    #    return pd.read_sql(query, conn)

    def fetch_data_push():
        parquet_file = 'Databases/DSSB_SE.UCS_HB_PUSH.parquet'
        return pd.read_parquet(parquet_file)

    #def fetch_data_extra_filters(extra_stream, conn):
    #    """Fetch user IDs from a specific table in the Oracle database."""
    #    query = f"select distinct g.iin from DSSB_DE.UCS_PUSH_OFF po left join dssb_se.gid_map2 g on po.owid = g.p_sid_ow left join DSSB_DE.DICT_UCS_EVENTS e on po.eventid = po.eventid where e.eventdescription like '%{extra_stream}%'"
    #    return pd.read_sql(query, conn)


    def fetch_data_extra_filters(selected_extra_streams):
        df = pd.read_parquet('Databases/DSSB_DE.UCS_PUSH_OFF.parquet')
       
        # Check what columns are available
        print(f"Available columns: {df.columns.tolist()}")
       
        if 'EVENTDESCRIPTION' in df.columns:
            filtered_df = df[df['EVENTDESCRIPTION'].isin(selected_extra_streams)]
        else:
            print("Warning: EVENTDESCRIPTION column not found. Using IIN column instead.")
            filtered_df = df[df['IIN'].isin(selected_extra_streams)]
       
        return filtered_df

    #def fetch_data_device(device, conn):
    #    """Fetch user IDs from a specific table in the Oracle database."""
    #    query = f"select distinct CLIENT_IIN IIN from dssb_dm.hb_sessions_fl h where h.SNAPSHOT_DATE> sysdate-2 and OPERATIONSYSTEM like '%{device}%'"
    #    return pd.read_sql(query, conn)

    def fetch_data_device(selected_devices):
        df = pd.read_parquet('Databases/dssb_dm.hb_sessions_fl.parquet')
        filtered_df = df[df['OPERATIONSYSTEM'].isin(selected_devices)]
        #st.info(selected_devices)
        return filtered_df

    def fetch_data_stream_target(stream, conn):
        """Fetch user IDs from a specific table in the Oracle database."""
        query = f"select distinct iin from dssb_ocds.mb22_local_target where lower(stream) like '%{stream}%' and date_end > sysdate + 0.5"
        return pd.read_sql(query, conn)

    def fetch_data_stream_control(stream, conn):
        """Fetch user IDs from a specific table in the Oracle database."""
        query = f"select distinct iin from DSSB_OCDS.mb21_local_control where lower(stream) like '%{stream}%' and date_end > sysdate + 0.5"
        return pd.read_sql(query, conn)

    def fetch_data_stream_control_target(stream, conn):
        """Fetch user IDs from a specific table in the Oracle database."""
        query = f"select distinct iin from DSSB_OCDS.mb21_local_control where lower(stream) like '%{stream}%' and date_end > sysdate + 0.5 union select distinct iin from dssb_ocds.mb22_local_target where lower(stream) like '%{stream}%' and date_end > sysdate + 0.5"
        return pd.read_sql(query, conn)


    def fetch_data_stream_target_rb3(stream_rb3, conn):
        query = f"select distinct t.iin from dssb_ocds.mb01_camp_dict d inner join dssb_ocds.mb22_local_target t on t.campaigncode = d.campaigncode where lower(d.sub_stream) like '%{stream_rb3}%' and date_end > sysdate + 0.5"
        return pd.read_sql(query, conn)

    def fetch_data_stream_control_rb3(strea_rb3, conn):
        query = f"select distinct t.iin from dssb_ocds.mb01_camp_dict d inner join dssb_ocds. mb22_local_control t on t.campaigncode = d.campaigncode where lower(d.sub_stream) like '%{stream_rb3}%' and date_end > sysdate + 0.5"
        return pd.read_sql(query, conn)

    def fetch_data_stream_control_target_rb3(stream_rb3, conn):
        query = f"""select distinct t.iin
from dssb_ocds.mb01_camp_dict d
inner join dssb_ocds.mb22_local_target t on t.campaigncode = d.campaigncode
where lower(d.sub_stream) like '%{stream_rb3}%' and date_end > sysdate + 0.5
union
select distinct t.iin
from dssb_ocds.mb01_camp_dict d
inner join dssb_ocds. mb22_local_control t on t.campaigncode = d.campaigncode
where lower(d.sub_stream) like '%{stream_rb3}%' and date_end > sysdate + 0.5"""
        return pd.read_sql(query, conn)



    def fetch_data_prev_campaign(prev_campaign, conn):
        query = f"select distinct CUSTOMER_IIN from dds.mc_campaign_fl where CAMPAIGNCODE = '{prev_campaign}'"
        return pd.read_sql(query, conn)

    def fetch_data_base_more_than_one(conn, date):
        """Fetch user IDs from a specific table in the Oracle database."""
        today_str = date.strftime("%d.%m.%Y")
        query = f"select f.IIN from spss_ocds.fd_rb2_campaigns_users f where f.upload_date = to_date('{today_str}','dd.mm.yyyy') group by f.IIN having count(*)>1"
        result_df = pd.read_sql(query, conn)
        
        # Ensure IIN column is properly formatted as string to maintain consistency
        if not result_df.empty and 'IIN' in result_df.columns:
            result_df['IIN'] = result_df['IIN'].astype(str)
            
        return result_df

    def fetch_phones(conn):
        query = f"select pr.iin, pr.phone from dssb_app.gidmap_contacts pr where pr.trusted = 1 and pr.actual = 1"
        return pd.read_sql(query, conn)


    def fetch_data_from_sql_query(conn, sql_script):
        query = sql_script
        return pd.read_sql(query, conn)

#    def filter_df_by_iin(df, conn):
#        cursor = conn.cursor()
#
#        cursor.execute("select iin from dssb_dev.dssb_push_analytics where hb_1m = 1")
#        iin_values = cursor.fetchall()
#      
#        # Convert fetched values to a set for faster lookup
#        iin_set = set(iin[0] for iin in iin_values)
#      
#        # Filter the DataFrame
#        filtered_df = df[df['IIN'].isin(iin_set)]
#      
#        return filtered_df

    def filter_df_by_iin(df):
   
        iin_values = pd.read_parquet('Databases/dssb_dev.dssb_push_analytics.parquet')
        iin_values = list(iin_values.itertuples(index = False, name = None))
       
        iin_set = set(iin[0] for iin in iin_values)
       
        # Filter the DataFrame
        filtered_df = df[df['IIN'].isin(iin_set)]
       
        return filtered_df
   
   
   
   
    def filter_dataframe(df, conn, selected_stop_list_columns,user_id_col_name, user_ids):
        """
        Filter the DataFrame by excluding rows based on IINs that meet
        stop list or blacklist criteria in the SQL database.
        """
        excluded_IINs = fetch_IINs_to_exclude(conn, selected_stop_list_columns, user_id_col_name, user_ids, user_limit)
   
        # Proceed with filtering if there are IINs to exclude
        if excluded_IINs:
            df = df[~df['IIN'].isin(excluded_IINs)]
           
       
        return df


    def filter_dataframe_two(df, conn, selected_stop_list_columns,user_id_col_name, user_ids):

        cursor.execute("""
        INSERT INTO bn_test (IIN)
        VALUES (:user_ids)
        """)

        cursor.execute("""
        select IIN from bn_test
        where IIN not in (select distinct iin from ACRM_DW.RB_BLACK_LIST@ACRM)
        """)
       
           
       
        return df
   
   
    def stratify_dataframe_and_ks_test(df, stratify_cols, train_size):
        st.write("–ò–∑–Ω–∞—á–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä DF:", df.shape)
       
        df = df.dropna(subset=stratify_cols)
        st.write("–†–∞–∑–º–µ—Ä DF –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –æ—Ç NULL:", df.shape)
   
        grouped = df.groupby(stratify_cols).filter(lambda x: len(x) > 1)
        st.write("–†–∞–∑–º–µ—Ä DF –ø–æ—Å–ª–µ –æ—á–∏—Å–∫–∏ –µ–¥–∏–Ω–∏—á–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤:", grouped.shape)
   
        if grouped.empty:
            st.write("No valid combinations left after filtering. Consider adjusting stratification columns.")
            return None, None, "No valid combinations left after filtering."
   
        rand_int = random.randint(1, 101)
   
        try:
            df1, df2 = train_test_split(grouped, train_size=train_size, random_state=rand_int, stratify=grouped[stratify_cols])
            st.write("–†–∞–∑–º–µ—Ä—ã DF1 –∏ DF2:", df1.shape, df2.shape)
        except ValueError as e:
            st.write("Error during train_test_split:", e)
            return None, None, f"Error during stratification: {e}"
   
        ks_test_results = []
        for col in grouped.columns:
            if grouped[col].dtype.kind in 'bifc' and col not in stratify_cols:
                if grouped[col].isnull().any():
                    st.write(f"Skipping column {col} due to NaN values.")
                    continue
                stat, p_value = ks_2samp(df1[col].dropna(), df2[col].dropna())
                ks_test_results.append({'Column': col, 'KS statistic': stat, 'p-value': p_value})
   
        return df1, df2, ks_test_results
       
   
   
    def increment_string(s):
        prefix = s[0]  # Get the first character
        number_str = s[1:]  # Get the numeric part as a string
        number = int(number_str)  # Convert the numeric part to an integer
        incremented_number = number + 1  # Increment the number
        incremented_number_str = str(incremented_number).zfill(len(number_str))  # Convert back to string, preserving leading zeros
        return f"{prefix}{incremented_number_str}"

    def increment_string_XLS(s):
        prefix = s[0]  # Get the first character
        prefix1 = s[1]  # Get the first character
        prefix2 = s[2]  # Get the first character
        prefix3 = s[3]  # Get the first character
        number_str = s[4:]  # Get the numeric part as a string
        number = int(number_str)  # Convert the numeric part to an integer
        incremented_number = number + 1  # Increment the number
        incremented_number_str = str(incremented_number).zfill(len(number_str))  # Convert back to string, preserving leading zeros
        return f"{prefix}{prefix1}{prefix2}{prefix3}{incremented_number_str}"



    def add_column_sum(df, selected_columns):
        if not all(column in df.columns for column in selected_columns):
            raise ValueError('One or more selected columns not in a dataframe')

        df['Column_sum'] = df[selected_columns].sum(axis=1)
        return df
   
   
   
   
    def merge_summed_data_with_df(original_df, summed_df, user_id_col_name):
        # Merge the original DataFrame with the summed DataFrame on user_id
        #st.write('//////////////////////////////////////////////////////////////////')
        #st.write(original_df)
        #st.write(summed_df)
        #st.write(user_id_col_name)
        merged_df = pd.merge(original_df, summed_df, on=user_id_col_name, how='left')
        return merged_df
   
   
    # Add function to load products from parquet file
    def load_products_from_parquet(parquet_file_path):
        """Load product data from parquet file"""
        try:
            parquet_base_dir = os.getenv('PARQUET_OUTPUT_DIR', 'Databases')
            full_path = os.path.join(parquet_base_dir, parquet_file_path)
           
            if not os.path.exists(full_path):
                st.error(f"Product file not found at {full_path}")
                return pd.DataFrame()
           
            df = pd.read_parquet(full_path)

            if "iin" in df.columns and 'IIN' not in df.columns:
                df = df.rename(columns={'iin': 'IIN'})
                st.info('Renamed column iin to IIN for consistency')

            # Ensure IIN data consistency
            if 'IIN' in df.columns:
                df['IIN'] = df['IIN'].astype(str).str.strip()

           
            return df
        except Exception as e:
            st.error(f"Error loading product data: {e}")
            return pd.DataFrame()

    def get_available_products(products_df):
        """Extract unique products from the products dataframe"""
        if 'sku_level1' in products_df.columns:
            return sorted(products_df['sku_level1'].dropna().unique().tolist())
        else:
            st.error("Product column 'sku_level1' not found in the data")
            return []

    def filter_by_products(products_df, selected_products):
        """Filter products dataframe by selected products"""
        if not selected_products:
            return products_df
       
        return products_df[products_df['sku_level1'].isin(selected_products)]

    def merge_with_psid(products_df):
        """Merge products dataframe with P_SID from main database"""
        try:
            psid_df = get_psid()  # Get IIN to P_SID mapping
            merged_df = pd.merge(products_df, psid_df, on='IIN', how='inner')
            return merged_df
        except Exception as e:
            st.error(f"Error merging with P_SID: {e}")
            return products_df

    data_source = st.radio(
        "–í—ã–±–µ—Ä–∏—Ç–µ –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è UserID:",
        ('–†–ë –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–ø—É—Å–∫', '–ì–∏–ø–µ—Ä–ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è', '–ü—Ä–æ–¥—É–∫—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞')
    )

    excel_toggle = st.toggle("Excel")

    tables = ['DSSB_OCDS.mb11_global_control', 'ACRM_DW.RB_BLACK_LIST@ACRM', 'dssb_de.dim_clients_black_list', 'SPSS_USER_DRACRM.HALYK_JOB@SPSS_LNK', 'SPSS_USER_DRACRM.BLOGGERS@SPSS_LNK',  'dssb_app.not_recommend_credits', 'BL_No_worker', 'dssb_app.abc_nbo_only', 'dssb_app.abc_ptb_models', 'dssb_app.abc_nbo_and_market']
    #selected_tables = st.multiselect('Select tables to query:', tables)

#'DSSB_OCDS.mb21_local_control', 'DSSB_OCDS.mb22_local_target'

    #st.sidebar.write(f'–í—ã –≤–æ—à–ª–∏ –∫–∞–∫: {st.session_state['username']}')
   
    # Initialize session state
    if 'open_new_tab' not in st.session_state:
        st.session_state.open_new_tab = False
   
    def set_open_new_tab():
        st.session_state.open_new_tab = True
   
    #st.button("AI helper", on_click=set_open_new_tab)
   
    # Check if we should open a new tab
    if st.session_state.open_new_tab:
        js = """
        <script>
        window.open('http://172.28.80.18:8525/&#39;, '_blank');
        </script>
        """
        st.components.v1.html(js, height=0, width=0)
        st.session_state.open_new_tab = False

   
    selected_tables = []

    checker = st.sidebar.toggle('–ö—Ç–æ –∑–∞–ø—É—Å—Ç–∏–ª –∫–∞–º–ø–∞–Ω–∏—é?')
    with st.sidebar:
        if checker:
            main_checker()
           

    st.sidebar.write("-------------------------------")
    st.sidebar.title("–°—Ç–æ–ø –ª–∏—Å—Ç—ã")

    for table in tables:
        is_selected = st.sidebar.toggle(table, key=table, value = 1)

        if is_selected:
            selected_tables.append(table)

    st.sidebar.write("-------------------------------")
   
    pushToggled = st.sidebar.toggle('Push –æ—Ç–∫–ª—é—á–µ–Ω')

    Extra_Filters = st.sidebar.toggle('Extra_Filters')

    device_filtered = st.sidebar.toggle('Device')

    if device_filtered:

        devices = ['android', 'iOS']

        selected_devices = []

        for device in devices:
            is_selected_devices = st.sidebar.checkbox(device, key=device)

            if is_selected_devices:
                selected_devices.append(device)


    if Extra_Filters:

        extra_streams = ['–ü–µ—Ä–µ–≤–æ–¥—ã', '–ü–ª–∞—Ç–µ–∂–∏', 'Kino.kz', '–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å', '–ú–∞—Ä–∫–µ—Ç', 'OTP/3ds/–ø–∞—Ä–æ–ª–∏', '–ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏', '–ö—Ä–µ–¥–∏—Ç—ã', '–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ë–∞–Ω–∫–∞', '–î–µ–ø–æ–∑–∏—Ç—ã', 'Onlinebank', 'Halyk —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ', 'Homebank', '–ë–æ–Ω—É—Å—ã', '–ì–æ—Å—É—Å–ª—É–≥–∏', 'Operation failed HalykTravel', '–û–±—â–∏–π –ø—É—à', 'O–ø–µ—Ä–∞—Ü–∏–∏ –ø–æ —Å—á–µ—Ç–∞–º']

        selected_extra_streams = []

        for extra_stream in extra_streams:
            is_selected_extra_stream = st.sidebar.checkbox(extra_stream, key=extra_stream)

            if is_selected_extra_stream:
                selected_extra_streams.append(extra_stream)

   
    st.sidebar.write("-------------------------------")
    local_control = st.sidebar.toggle('DSSB_OCDS.mb21_local_control')
    local_target = st.sidebar.toggle('DSSB_OCDS.mb22_local_target')

    if local_control or local_target:

        streams = ['market', 'general', 'travel', 'govtech', 'credit', 'insurance', 'deposit', 'kino', 'transactions', 'hm']
   
        selected_streams = []
       
        for stream in streams:
            is_selected = st.sidebar.checkbox(stream, key=stream)
   
            if is_selected:
                selected_streams.append(stream)



    st.sidebar.write("-------------------------------")
    local_control_rb3 = st.sidebar.toggle('DSSB_OCDS.mb21_local_control_RB3')
    local_target_rb3 = st.sidebar.toggle('DSSB_OCDS.mb22_local_target_RB3')

    if local_control_rb3 or local_target_rb3:

        streams_rb3 = ['–±–∑–∫', '–±–∏–∑–Ω–µ—Å –∏ –Ω–∞–ª–æ–≥–∏', 'concert', '–Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å', 'transfers', '–≤—Å–µ', 'aqyl', '–æ–±—â–∞—è', '—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç', 'bnpl', 'card', 'realtime', 'theatr', '–∑–¥–æ—Ä–æ–≤—å—è', '–ø–ª–∞—à–µ—Ç—ã', 'cards', '–Ω–æ—Ç–∞—Ä–∏—É—Å –∏ —Ü–æ–Ω', '–ª—å–≥–æ—Ç—ã, –ø–æ—Å–æ–±–∏–µ, –ø–µ–Ω—Å–∏–∏', 'halykapp', '–∞–≤—Ç–æ–∫—Ä–µ–¥–∏—Ç', '–∏–ø–æ—Ç–µ–∫–∞', 'kino', '—Ü–∏—Ñ—Ä–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã']
   
        selected_streams_rb3 = []
       
        for stream_rb3 in streams_rb3:
            is_selected_rb3 = st.sidebar.checkbox(stream_rb3, key=stream_rb3)
   
            if is_selected_rb3:
                selected_streams_rb3.append(stream_rb3)

    if 'df' not in st.session_state:
        st.session_state.df = pd.DataFrame()

    if 'dfnew' not in st.session_state:
        st.session_state.dfnew = 0

    if 'dfdelta' not in st.session_state:
        st.session_state.dfdelta = 0

    if 'load' not in st.session_state:
        st.session_state.load = pd.DataFrame()

    with st.sidebar:

        if not st.session_state.df.empty:
            num_rows = st.session_state.df .shape[0]

            if st.session_state.dfnew != num_rows:
                st.session_state.dfdelta = num_rows - st.session_state.dfnew
                st.session_state.dfnew = num_rows

            value_main = f"{st.session_state.dfnew:,}"
            value_sub = f"{st.session_state.dfdelta:,}"
           
            st.metric(label="–ó–∞–ø–∏—Å–∏ –≤ –±–∞–∑–µ:", value=value_main, delta=value_sub)
           
        else:
            st.metric(label="–ó–∞–ø–∏—Å–∏ –≤ –±–∞–∑–µ:", value="No Data", delta="No Data")

        style_metric_cards()

        if st.button('–û–±–Ω–æ–≤–∏—Ç—å –≤–∏–¥–∂–µ—Ç'):
            st.rerun()

    if excel_toggle:
        loaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –¥–∞–Ω–Ω—ã—Ö (excel, csv)", type=['xlsx', 'csv'])

        if loaded_file:
            file_extension = loaded_file.name.split('.')[-1].lower()
           
            if file_extension == "csv":
                st.session_state.load = pd.read_csv(loaded_file)
            else:
                st.session_state.load = pd.read_excel(loaded_file)

            st.session_state.load['IIN'] = st.session_state.load['IIN'].astype(str).str.replace(',', '')

        check_password_for_sql = '123'
       
        password_for_sql = st.text_input("password")
       
        if password_for_sql == check_password_for_sql:
            sql_script = st.text_area("Sql to execute for(DSSB): ")

            if st.button('Load data from sql') and sql_script:

                with get_connection_DSSB_OCDS() as conn:
                    st.session_state.load = fetch_data_from_sql_query(conn, sql_script)


        if not st.session_state.load.empty:
            st.write(st.session_state.load.head(500))
            num_rows = st.session_state.load.shape[0]
            st.success(f"–í—ã–≥—Ä—É–∂–µ–Ω–Ω–æ: {num_rows}")
       
   
   
    if data_source == '–†–ë –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–ø—É—Å–∫' and not excel_toggle:
        selected_info_columns = st.multiselect('–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∏–ª—å—Ç—Ä—ã', information_columns, default = ['SNAPSHOT_DATE', 'P_SID', 'IIN', 'PUBLIC_ID', 'IS_MAU'])
        #selected_sum_columns = []
        selected_sum_columns = st.multiselect('–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∏–ª—å—Ç—Ä—ã', sum_columns)
        #selected_stop_list_columns = st.multiselect('–í—ã–±–µ—Ä–∏—Ç–µ –°—Ç–æ–ø –ª–∏—Å—Ç—ã', ALL_STOP_LIST)
   
       
        #load_option = st.selectbox("–°–∫–æ–ª—å–∫–æ –∑–∞–≥—Ä—É–∂–∞—Ç—å?:", ["–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ", "–í—ã–±—Ä–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ"])
       
        #if load_option == "–í—ã–±—Ä–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ":
        #    user_limit = st.number_input("–í—ã–±–µ—Ä–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏–Ω–∏–π –∫ –∑–∞–≥—Ä—É–∑–∫–µ:", min_value=1, value=20)
        #else:
        user_limit = None  # None or a special value indicating all records should be loaded
       
        #////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
        # if statements for filters
        # Additional Filters based on user selection
        additional_filters = {}
        if 'AGE' in selected_info_columns:
            st.subheader('AGE')
            min_age, max_age = st.slider('–í—ã–±–µ—Ä–∏—Ç–µ –≤–æ–∑—Ä–∞—Å—Ç', 0, 100, (18, 60))
            additional_filters['AGE'] = ('BETWEEN', min_age, max_age)
       
       
       
       
        # Define a list of dictionaries containing column information
        info_columns = [
            {'column_name': 'LEGAL_FORM', 'display_name': 'LEGAL_FORM'},
            {'column_name': 'BUSINESS_TYPE', 'display_name': 'BUSINESS_TYPE'},
            {'column_name': 'BRANCH_NAME', 'display_name': 'BRANCH_NAME'},
            {'column_name': 'RISK_CATEGORY', 'display_name': 'RISK_CATEGORY'},
            {'column_name': 'OFFER', 'display_name': 'OFFER'},
            {'column_name': 'KATO', 'display_name': 'KATO'},
            {'column_name': 'BUSINESS_TYPE_REASON', 'display_name': 'BUSINESS_TYPE_REASON'},
            {'column_name': 'PREV_CAMP_ALL', 'display_name': 'PREV_CAMP_ALL'},
            {'column_name': 'PREV_CAMP_FIZ', 'display_name': 'PREV_CAMP_FIZ'},
            {'column_name': 'RANK_AQUIR', 'display_name': 'RANK_AQUIR'},
            {'column_name': 'RANK_FX', 'display_name': 'RANK_FX'}
   
   
        ]
       
        for column_info in info_columns:
            column_name = column_info['column_name']
            display_name = column_info['display_name']
           
            if column_name in selected_info_columns:
                st.subheader(display_name)
                with get_connection() as conn:
                    query = f'SELECT DISTINCT {column_name} FROM dssb_app.rb_feature_store WHERE {column_name} IS NOT NULL ORDER BY {column_name}'
                    results = pd.read_sql(query, conn)[column_name].tolist()
               
                selected_options = st.multiselect(f'–í—ã–±–µ—Ä–∏—Ç–µ {display_name}', options=results)
               
                if selected_options:
                    additional_filters[column_name] = ('IN', selected_options)
       
        #////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
        # Initialize session state for storing the DataFrame if it's not already initialized
        if 'df' not in st.session_state:
            st.session_state.df = pd.DataFrame()

    # New data source: Product-based selection
    elif data_source == '–ü—Ä–æ–¥—É–∫—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞' and not excel_toggle:
        st.subheader('–ü—Ä–æ–¥—É–∫—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞')
       
        # Initialize session state for products
        if 'products_df' not in st.session_state:
            st.session_state.products_df = pd.DataFrame()
       
        if st.button('–ó–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ'):
            st.session_state.products_df = load_products_from_parquet("final.parquet")
            if not st.session_state.products_df.empty:
                st.success(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(st.session_state.products_df)} –∑–∞–ø–∏—Å–µ–π —Å –ø—Ä–æ–¥—É–∫—Ç–∞–º–∏")
                st.write("–ü—Ä–µ–≤—å—é –¥–∞–Ω–Ω—ã—Ö:")
                st.write(st.session_state.products_df.head())
            else:
                st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ –ø—Ä–æ–¥—É–∫—Ç–∞—Ö")
       
        # Product selection interface
        if not st.session_state.products_df.empty:
            available_products = get_available_products(st.session_state.products_df)
           
            if available_products:
                selected_products = st.multiselect(
                    '–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–æ–¥—É–∫—Ç—ã –¥–ª—è –∫–∞–º–ø–∞–Ω–∏–∏:',
                    options=available_products,
                    default=available_products[:3] if len(available_products) >= 3 else available_products,
                    help="–í—ã–±–µ—Ä–∏—Ç–µ –æ–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –¥–ª—è —Ç–∞—Ä–≥–µ—Ç–∏–Ω–≥–∞"
                )
               
                if selected_products:
                    # Filter by selected products
                    filtered_products_df = filter_by_products(st.session_state.products_df, selected_products)
                   
                    st.info(f"–í—ã–±—Ä–∞–Ω–æ {len(filtered_products_df)} –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å –ø—Ä–æ–¥—É–∫—Ç–∞–º–∏: {', '.join(selected_products)}")
                   
                    # Display statistics by product
                    if len(selected_products) > 1:
                        product_stats = filtered_products_df['sku_level1'].value_counts()
                        st.write("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º:")
                        st.bar_chart(product_stats)
                   
                    # Store filtered data for further processing
                    st.session_state.filtered_products_df = filtered_products_df
                else:
                    st.warning("–í—ã–±–µ—Ä–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –ø—Ä–æ–¥—É–∫—Ç –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è")
            else:
                st.error("–ü—Ä–æ–¥—É–∫—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
       
        # Set variables for compatibility with existing logic
        user_limit = None
        additional_filters = {}
        selected_info_columns = ['IIN', 'P_SID']  # Minimal required columns
        selected_sum_columns = []
       
        # Initialize session state for storing the DataFrame if it's not already initialized
        if 'df' not in st.session_state:
            st.session_state.df = pd.DataFrame()
           

    else:
        user_limit = None
        additional_filters = {}
        selected_info_columns = ['IIN', 'P_SID']
        selected_sum_columns = []
        if 'df' not in st.session_state:
            st.session_state.df = pd.DataFrame()
       
    if not excel_toggle:
       
        if st.button('–ó–∞–ø—Ä–æ—Å –∫ –±–∞–∑–µ'):
            with st.status('–ò–¥–µ—Ç –∑–∞–≥—Ä—É–∑–∫–∞ —Å –±–∞–∑—ã, –æ–∂–∏–¥–∞–π—Ç–µ...', expanded = True) as status:
               
                # Special handling for product-based data source
                if data_source == '–ü—Ä–æ–¥—É–∫—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞':
                    if 'filtered_products_df' not in st.session_state or st.session_state.filtered_products_df.empty:
                        st.error("–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∏ –≤—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–æ–¥—É–∫—Ç—ã!")
                        status.update(label='–û—à–∏–±–∫–∞: –ø—Ä–æ–¥—É–∫—Ç—ã –Ω–µ –≤—ã–±—Ä–∞–Ω—ã', state='error', expanded=False)
                        st.stop()
                   
                    # Use product-filtered data as the base
                    df = st.session_state.filtered_products_df.copy()
                    
                    # Ensure IIN consistency for product data
                    if 'IIN' in df.columns:
                        df['IIN'] = df['IIN'].astype(str).str.strip()
                   
                    # Merge with P_SID if not already present
                    if 'P_SID' not in df.columns:
                        df = merge_with_psid(df)
                        st.write('P_SID –¥–æ–±–∞–≤–ª–µ–Ω –∫ –ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º')
                   
                    st.write('–ü—Ä–æ–¥—É–∫—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∫–∞–∫ –æ—Å–Ω–æ–≤–∞')
                    user_ids = df['IIN'].tolist()
                    table_name = 'product_based'
                    user_id_col_name = 'IIN'
                   
                # Original logic for other data sources
                else:
                    if selected_tables == []:
                        st.error('üö®üö®üö® –í—ã –Ω–µ –≤—ã–±—Ä–∞–ª–∏ –Ω–∏ –æ–¥–Ω–æ–≥–æ Stop –∏–ª–∏ Black –ª–∏—Å—Ç–∞! –í—ã —É–≤–µ—Ä–µ–Ω–Ω—ã? üö®üö®üö®')

                        table_name = 'dssb_app.rb_feature_store'
                        user_id_col_name = 'IIN'
                        if user_limit != None:

                            all_columns = selected_info_columns + selected_sum_columns
                            #df = query_database(all_columns, additional_filters, user_limit)
                            df = pd.read_parquet('Databases/dssb_app.rb_feature_store.parquet')
                            df = df[all_columns]
                            st.write('–û—Å–Ω–æ–≤–Ω–∞—è –±–∞–∑–∞ –ø–æ–¥–≥—Ä—É–∂–µ–Ω–∞')

                        if user_limit == None:
                            #st.info('1')
                            all_columns = selected_info_columns + selected_sum_columns
                            #df = query_database_all(all_columns, additional_filters)
                            df = pd.read_parquet('Databases/dssb_app.rb_feature_store.parquet')
                            df = df[all_columns]
                            st.write('–û—Å–Ω–æ–≤–Ω–∞—è –±–∞–∑–∞ –ø–æ–¥–≥—Ä—É–∂–µ–Ω–∞')
                            #st.info('2')
                        user_ids = df['IIN'].tolist()

                        if selected_sum_columns != []:
                            # Fetch summed columns from SQL

                            summed_df = add_column_sum(df, selected_sum_columns)
                    else:
                        table_name = 'dssb_app.rb_feature_store'
                        user_id_col_name = 'IIN'
                        if user_limit != None:

                            all_columns = selected_info_columns + selected_sum_columns
                            #df = query_database(all_columns, additional_filters, user_limit)
                            df = pd.read_parquet('Databases/dssb_app.rb_feature_store.parquet')
                            df = df[all_columns]
                            st.write('–û—Å–Ω–æ–≤–Ω–∞—è –±–∞–∑–∞ –ø–æ–¥–≥—Ä—É–∂–µ–Ω–∞')

                        if user_limit == None:
                            all_columns = selected_info_columns + selected_sum_columns
                            #df = query_database_all(all_columns, additional_filters)
                            df = pd.read_parquet('Databases/dssb_app.rb_feature_store.parquet')
                            df = df[all_columns]
                            st.write('–û—Å–Ω–æ–≤–Ω–∞—è –±–∞–∑–∞ –ø–æ–¥–≥—Ä—É–∂–µ–Ω–∞')
                        user_ids = df['IIN'].tolist()

                        if selected_sum_columns != []:
                            # Fetch summed columns from SQL

                            summed_df = add_column_sum(df, selected_sum_columns)

                    if selected_sum_columns != []:
                        df = summed_df

                #st.write(df)

                num_rows = df.shape[0]

                if not selected_tables:

                    st.success("–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.")
                    st.success(f"–í –±–∞–∑–µ {num_rows} –∑–∞–ø–∏—Å–µ–π")
                    st.session_state.df = df

               
   
                if selected_tables:
                    # Example usage within your main application
                    with get_connection_DSSB_OCDS() as conn:
                        #df = filter_dataframe(df, conn, selected_stop_list_columns,user_id_col_name, user_ids)
                        # Fetch and combine user IDs from selected tables
                        all_user_ids = pd.DataFrame()
                        for table in selected_tables:
                            fetched_ids = fetch_data(table)
                            all_user_ids = pd.concat([all_user_ids, fetched_ids], ignore_index=True)
                            st.write(f'–°—Ç–æ–ø –ª–∏—Å—Ç {table} –æ—á–∏—â–µ–Ω')

                        if local_control and not local_target:
                            for stream in selected_streams:
                                fetched_ids_extra = fetch_data_stream_control(stream, conn)
                                all_user_ids = pd.concat([all_user_ids, fetched_ids_extra], ignore_index=True)  
                                st.write(f'local_control {stream} –æ—á–∏—â–µ–Ω')

                        if local_target and not local_control:
                            for stream in selected_streams:
                                fetched_ids_extra = fetch_data_stream_target(stream, conn)
                                all_user_ids = pd.concat([all_user_ids, fetched_ids_extra], ignore_index=True)  
                                st.write(f'local_target {stream} –æ—á–∏—â–µ–Ω')

                        if local_control and local_target:
                            for stream in selected_streams:
                                fetched_ids_extra = fetch_data_stream_control_target(stream, conn)
                                all_user_ids = pd.concat([all_user_ids, fetched_ids_extra], ignore_index=True)  
                                st.write(f'local_control –∏ local_target {stream} –æ—á–∏—â–µ–Ω')





                        if local_control_rb3 and not local_target_rb3:
                            for stream_rb3 in selected_streams_rb3:
                                fetched_ids_extra_rb3 = fetch_data_stream_control_rb3(stream_rb3, conn)
                                all_user_ids = pd.concat([all_user_ids, fetched_ids_extra_rb3], ignore_index=True)  
                                st.write(f'local_control {stream_rb3} –æ—á–∏—â–µ–Ω_rb3')

                        if local_target_rb3 and not local_control_rb3:
                            for stream_rb3 in selected_streams_rb3:
                                fetched_ids_extra_rb3 = fetch_data_stream_target_rb3(stream_rb3, conn)
                                all_user_ids = pd.concat([all_user_ids, fetched_ids_extra_rb3], ignore_index=True)  
                                st.write(f'local_target {stream_rb3} –æ—á–∏—â–µ–Ω_rb3')

                        if local_control_rb3 and local_target_rb3:
                            for stream_rb3 in selected_streams_rb3:
                                fetched_ids_extra_rb3 = fetch_data_stream_control_target_rb3(stream_rb3, conn)
                                all_user_ids = pd.concat([all_user_ids, fetched_ids_extra_rb3], ignore_index=True)  
                                st.write(f'local_control_rb3 –∏ local_target_rb3 {stream_rb3} –æ—á–∏—â–µ–Ω')



                       

                        if pushToggled:
                            #st.info('toggled push')
                            fetched_ids_push = fetch_data_push()
                            all_user_ids = pd.concat([all_user_ids, fetched_ids_push], ignore_index=True)
                            st.write('Push –æ—á–∏—â–µ–Ω')

                        if Extra_Filters:
                            #for extra_stream in selected_extra_streams:
                            fetched_ids_extra_filters = fetch_data_extra_filters(selected_extra_streams)
                            all_user_ids = pd.concat([all_user_ids, fetched_ids_extra_filters], ignore_index=True)
                            st.write(f'Extra filters {selected_extra_streams} –æ—á–∏—â–µ–Ω—ã')

                        if device_filtered:
                            #for device in selected_devices:
                            fetched_ids_devices = fetch_data_device(selected_devices)
                            all_user_ids = pd.concat([all_user_ids, fetched_ids_devices], ignore_index=True)  
                            st.write(f'Devise {selected_devices} –æ—á–∏—â–µ–Ω')
                           
                           
                       
                        # Remove duplicates across all fetched IDs
                        all_user_ids = all_user_ids.drop_duplicates()
       
                        # Assume df is defined elsewhere and has column 'IIN'
                        original_count = len(df)
                        df_filtered = df[~df['IIN'].isin(all_user_ids['IIN'])]
                        removed_count = original_count - len(df_filtered)
                       
                        st.success(f'Removed {removed_count} users from df.')
                                                   
                        num_rows = df_filtered.shape[0]
   
                   
                        st.success("–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.")
                        st.success(f"–í –±–∞–∑–µ {num_rows} –∑–∞–ø–∏—Å–µ–π –ø–æ—Å–ª–µ –±–ª—ç–∫ –∏ —Å—Ç–æ–ø –ª–∏—Å—Ç–æ–≤")
                        st.session_state.df = df_filtered

                        status.update(
                            label = '–ë–∞–∑–∞ –ì–æ—Ç–æ–≤–∞.', state = 'complete', expanded = False
                        )

                        #df_filtered.to_parquet('Databases/DF_final.parquet', compression='snappy')

            #st.experimental_rerun()

    elif excel_toggle:
        if st.button('–û—á–∏—Å—Ç–∫–∞ —Ñ–∞–π–ª–∞'):
            with st.status('–ò–¥–µ—Ç –∑–∞–≥—Ä—É–∑–∫–∞ —Å –±–∞–∑—ã, –æ–∂–∏–¥–∞–π—Ç–µ...', expanded = True) as status:
                df = st.session_state.load

                st.write('–î–æ–±–∞–≤–ª—è–µ–º PSID')
                psid_df = get_psid()
                st.write('PSID –¥–æ–±–∞–≤–ª–µ–Ω')
                if selected_tables == []:
                    st.error('üö®üö®üö® –í—ã –Ω–µ –≤—ã–±—Ä–∞–ª–∏ –Ω–∏ –æ–¥–Ω–æ–≥–æ Stop –∏–ª–∏ Black –ª–∏—Å—Ç–∞! –í—ã —É–≤–µ—Ä–µ–Ω–Ω—ã? üö®üö®üö®')
   
                    table_name = 'dssb_app.rb_feature_store'
                    user_id_col_name = 'IIN'
                    #df = pd.read_excel(loaded_file)
                    original_count = len(df)
                    df['IIN'] = df['IIN'].astype(str).str.replace(',', '')
                    df = pd.merge(df, psid_df, on='IIN', how='inner')
                    user_ids = df['IIN'].tolist()
       
                else:
                    table_name = 'dssb_app.rb_feature_store'
                    user_id_col_name = 'IIN'
                    #df = pd.read_excel(loaded_file)
                    original_count = len(df)
                    df['IIN'] = df['IIN'].astype(str).str.replace(',', '')
                    df = pd.merge(df, psid_df, on='IIN', how='inner')
                    user_ids = df['IIN'].tolist()
   
                    num_rows = df.shape[0]
   
                if not selected_tables:

                    st.success("–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.")
                    st.success(f"–í –±–∞–∑–µ {num_rows} –∑–∞–ø–∏—Å–µ–π")
                    st.session_state.df = df


   
                if selected_tables:
                    # Example usage within your main application
                    with get_connection_DSSB_OCDS() as conn:
                        #df = filter_dataframe(df, conn, selected_stop_list_columns,user_id_col_name, user_ids)
                        # Fetch and combine user IDs from selected tables
                        all_user_ids = pd.DataFrame()
                        for table in selected_tables:
                            fetched_ids = fetch_data(table)
                            all_user_ids = pd.concat([all_user_ids, fetched_ids], ignore_index=True)
                            st.write(f'–°—Ç–æ–ø –ª–∏—Å—Ç {table} –æ—á–∏—â–µ–Ω')

                        if local_control and not local_target:
                            for stream in selected_streams:
                                fetched_ids_extra = fetch_data_stream_control(stream, conn)
                                all_user_ids = pd.concat([all_user_ids, fetched_ids_extra], ignore_index=True)  
                                st.write(f'local_control {stream} –æ—á–∏—â–µ–Ω')

                        if local_target and not local_control:
                            for stream in selected_streams:
                                fetched_ids_extra = fetch_data_stream_target(stream, conn)
                                all_user_ids = pd.concat([all_user_ids, fetched_ids_extra], ignore_index=True)  
                                st.write(f'local_target {stream} –æ—á–∏—â–µ–Ω')

                        if local_control and local_target:
                            for stream in selected_streams:
                                fetched_ids_extra = fetch_data_stream_control_target(stream, conn)
                                all_user_ids = pd.concat([all_user_ids, fetched_ids_extra], ignore_index=True)  
                                st.write(f'local_control –∏ local_target {stream} –æ—á–∏—â–µ–Ω')








                        if local_control_rb3 and not local_target_rb3:
                            for stream_rb3 in selected_streams_rb3:
                                fetched_ids_extra_rb3 = fetch_data_stream_control_rb3(stream_rb3, conn)
                                all_user_ids = pd.concat([all_user_ids, fetched_ids_extra_rb3], ignore_index=True)  
                                st.write(f'local_control {stream_rb3} –æ—á–∏—â–µ–Ω_rb3')

                        if local_target_rb3 and not local_control_rb3:
                            for stream_rb3 in selected_streams_rb3:
                                fetched_ids_extra_rb3 = fetch_data_stream_target_rb3(stream_rb3, conn)
                                all_user_ids = pd.concat([all_user_ids, fetched_ids_extra_rb3], ignore_index=True)  
                                st.write(f'local_target {stream_rb3} –æ—á–∏—â–µ–Ω_rb3')

                        if local_control_rb3 and local_target_rb3:
                            for stream_rb3 in selected_streams_rb3:
                                fetched_ids_extra_rb3 = fetch_data_stream_control_target_rb3(stream_rb3, conn)
                                all_user_ids = pd.concat([all_user_ids, fetched_ids_extra_rb3], ignore_index=True)  
                                st.write(f'local_control_rb3 –∏ local_target_rb3 {stream_rb3} –æ—á–∏—â–µ–Ω')



                       

                        if pushToggled:
                            #st.info('toggled push')
                            fetched_ids_push = fetch_data_push()
                            all_user_ids = pd.concat([all_user_ids, fetched_ids_push], ignore_index=True)
                            st.write('Push –æ—á–∏—â–µ–Ω')

                        if Extra_Filters:
                            #for extra_stream in selected_extra_streams:
                            fetched_ids_extra_filters = fetch_data_extra_filters(selected_extra_streams)
                            all_user_ids = pd.concat([all_user_ids, fetched_ids_extra_filters], ignore_index=True)
                            st.write(f'Extra filters {selected_extra_streams} –æ—á–∏—â–µ–Ω—ã')

                        if device_filtered:
                            #for device in selected_devices:
                            fetched_ids_devices = fetch_data_device(selected_devices)
                            all_user_ids = pd.concat([all_user_ids, fetched_ids_devices], ignore_index=True)  
                            st.write(f'Devise {selected_devices} –æ—á–∏—â–µ–Ω')
                           
                           
                       
                        # Remove duplicates across all fetched IDs
                        all_user_ids = all_user_ids.drop_duplicates()
       
                        # Assume df is defined elsewhere and has column 'IIN'
                        df_filtered = df[~df['IIN'].isin(all_user_ids['IIN'])]
                        removed_count = original_count - len(df_filtered)
                       
                        st.success(f'Removed {removed_count} users from df.')
                                                   
                        num_rows = df_filtered.shape[0]
   
                   
                        st.success("–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.")
                        st.success(f"–í –±–∞–∑–µ {num_rows} –∑–∞–ø–∏—Å–µ–π –ø–æ—Å–ª–µ –±–ª—ç–∫ –∏ —Å—Ç–æ–ø –ª–∏—Å—Ç–æ–≤")
                        st.session_state.df = df_filtered

                        status.update(
                            label = '–ë–∞–∑–∞ –ì–æ—Ç–æ–≤–∞.', state = 'complete', expanded = False
                        )

            #st.experimental_rerun()


    if not st.session_state.df.empty:
        st.write(st.session_state.df.head(500))
        num_rows = st.session_state.df.shape[0]
        st.success(f"–í –±–∞–∑–µ {num_rows}")

        def trim_dataframe(dataframe, max_rows):
            trimmed_df = dataframe.dropna().head(max_rows)
            return trimmed_df

        with st.expander(f"–û—á–∏—Å—Ç–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∫–∞–º–ø–∞–Ω–∏–π"):
           
            prev_campaign = campaign_id = st.text_input("–ù–æ–º–µ—Ä–∞ –∫–∞–º–ø–∞–Ω–∏–π –∫ –æ—á–∏—Å—Ç–∫–µ")

            if st.button("–û—á–∏—Å—Ç–∏—Ç—å –∫–ª–∏–µ–Ω—Ç–æ–≤"):
                st.write(prev_campaign)
                with get_connection_ED_OCDS() as conn:
                    all_user_ids_campaign = pd.DataFrame()
                    fetched_prev_campaign_users = fetch_data_prev_campaign(prev_campaign, conn)

                    all_user_ids_campaign = pd.concat([all_user_ids_campaign, fetched_prev_campaign_users], ignore_index=True)  

                    # Remove duplicates across all fetched IDs
                    all_user_ids_campaign = all_user_ids_campaign.drop_duplicates()
                    df = st.session_state.df

                    # Assume df is defined elsewhere and has column 'IIN'
                    original_count = len(df)
                    df_filtered = df[~df['IIN'].isin(all_user_ids_campaign['CUSTOMER_IIN'])]
                    removed_count = original_count - len(df_filtered)
                   
                    st.success(f'Removed {removed_count} users from df.')
                                               
                    num_rows = df_filtered.shape[0]

               
                    st.success("–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.")
                    st.success(f"–í –±–∞–∑–µ {num_rows} –∑–∞–ø–∏—Å–µ–π –ø–æ—Å–ª–µ –±–ª—ç–∫ –∏ —Å—Ç–æ–ø –ª–∏—Å—Ç–æ–≤")
                    st.session_state.df = df_filtered


           

        with st.expander(f"–ß–∏—Å—Ç–∫–∞ –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏"):
            DATE_FOR_CLEAN = st.date_input("–û—á–∏—Å—Ç–∫–∞ –ø–æ –¥–∞—Ç–µ:")

            if st.button("–û—á–∏—Å—Ç–∏—Ç—å —Ç–µ—Ö –∫—Ç–æ –≤ –±–∞–∑–µ campaigns_users > 1 —Ä–∞–∑"):
                all_user_ids = pd.DataFrame()
                original_count = len(st.session_state.df)
                with get_connection_SPSS_OCDS() as conn:
                    fetched_ids_more_than_one = fetch_data_base_more_than_one(conn, DATE_FOR_CLEAN)
                    all_user_ids = pd.concat([all_user_ids, fetched_ids_more_than_one], ignore_index=True)

                    all_user_ids = all_user_ids.drop_duplicates()

                    # Enhanced IIN comparison to handle data type consistency issues
                    # Convert both dataframes' IIN columns to string format for consistent comparison
                    if not all_user_ids.empty:
                        st.info(f"–ù–∞–π–¥–µ–Ω–æ {len(all_user_ids)} –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö IIN –≤ –±–∞–∑–µ –Ω–∞ {DATE_FOR_CLEAN}")
                        
                        # Normalize IINs in fetched data to strings without leading zeros for comparison
                        all_user_ids['IIN_normalized'] = all_user_ids['IIN'].astype(str).str.strip().str.lstrip('0')
                        
                        # Normalize IINs in session dataframe to strings without leading zeros
                        st.session_state.df['IIN_normalized'] = st.session_state.df['IIN'].astype(str).str.strip().str.lstrip('0')
                        
                        # Filter using normalized IINs
                        df_filtered = st.session_state.df[~st.session_state.df['IIN_normalized'].isin(all_user_ids['IIN_normalized'])]
                        
                        # Remove the temporary normalized column
                        df_filtered = df_filtered.drop('IIN_normalized', axis=1)
                        st.session_state.df = df_filtered
                    else:
                        st.info(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö IIN –≤ –±–∞–∑–µ –Ω–∞ {DATE_FOR_CLEAN}")
                        # If no duplicates found, no filtering needed
                        df_filtered = st.session_state.df

                    num_rows = df_filtered.shape[0]

                    removed_count = original_count - len(df_filtered)
                           
                    st.info(f'—É–¥–∞–ª–µ–Ω–Ω–æ {removed_count} –∏–∑ –±–∞–∑—ã –Ω–∞ {DATE_FOR_CLEAN}.')

               
                    st.success("–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.")
                    st.success(f"–í –±–∞–∑–µ {num_rows} –∑–∞–ø–∏—Å–µ–π")
   
            if st.button('–ó–∞–ø–∏—Å–∏ –≤ –±–∞–∑–µ fd_rb2_campaigns_users:'):
                with get_connection_SPSS_OCDS() as conn:
                    fetched_ids_more_than_one = fetch_data_base_more_than_one(conn, DATE_FOR_CLEAN)
                num = len(fetched_ids_more_than_one)
                value_main = f"{num:,}"
                st.metric(label=f"–ó–∞–ø–∏—Å–∏ –≤ –±–∞–∑–µ –Ω–∞ {DATE_FOR_CLEAN}:", value=value_main)
   
   
           
            style_metric_cards()
           
        with st.expander("–û—á–∏—Å—Ç–∫–∞"):

            # Button to remove rows where IS_MAU is 0
            if st.button('–û—á–∏—Å—Ç–∏—Ç—å –≤—Å–µ—Ö –≥–¥–µ MAU = 0  –∏–∑ dssb_dev.dssb_push_analytics'):
                with get_connection_DSSB_OCDS() as conn:
                    st.session_state.df = filter_df_by_iin(st.session_state.df)
       
                    num_rows = st.session_state.df.shape[0]
                    st.success("MAU —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω")
                    st.success(f"–í –±–∞–∑–µ {num_rows} –∑–∞–ø–∏—Å–µ–π")
                    st.write(st.session_state.df.head(500))
                    #st.experimental_rerun()
           
            # Button to remove rows where PUBLIC_ID is None or empty
            if st.button('–û—á–∏—Å—Ç–∏—Ç—å –≤—Å–µ—Ö –≥–¥–µ PUBLIC_ID = 0 –∏–ª–∏ None'):
                st.session_state.df = st.session_state.df[st.session_state.df['PUBLIC_ID'].notna() & (st.session_state.df['PUBLIC_ID'] != '') & (st.session_state.df['PUBLIC_ID'] != 'None')]
                st.dataframe(st.session_state.df)
   
                num_rows = st.session_state.df.shape[0]
                st.success("PUBLIC_ID —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω")
                st.success(f"–í –±–∞–∑–µ {num_rows} –∑–∞–ø–∏—Å–µ–π")
                #st.experimental_rerun()

            if st.button('–°–∫–æ–ª—å–∫–æ –≤ –ø–æ–∫—Ä—ã—Ç–∏–µ –ú–êU?'):
                parquet_file = 'Databases/MAU.parquet'
                df_mau = pd.read_parquet(parquet_file)
                num_rows = df_mau.shape[0]
                st.info(f"–í –±–∞–∑–µ MAU {num_rows} –∑–∞–ø–∏—Å–µ–π")

            if st.button('–°–æ–≤–º–µ—Å—Ç–∏—Ç—å –≤—ã–±–æ—Ä–∫—É —Å –ø–æ–∫—Ä—ã—Ç–∏–µ–º MAU'):
                parquet_file = 'Databases/MAU.parquet'
                df_mau = pd.read_parquet(parquet_file)
                iins_to_keep = set(df_mau['IIN'])
                st.session_state.df = st.session_state.df[st.session_state.df['IIN'].isin(iins_to_keep)]
                num_rows = st.session_state.df.shape[0]
                st.success(f"–í –±–∞–∑–µ {num_rows} –∑–∞–ø–∏—Å–µ–π")

            st.write('-----------------------------------------------------------')


            filial_list = [
'–¢—É—Ä–∫–µ—Å—Ç–∞–Ω—Å–∫–∏–π –û–§'
,'–ü–∞–≤–ª–æ–¥–∞—Ä—Å–∫–∏–π –û–§'
,'–°–µ–≤–µ—Ä–æ-–ö–∞–∑–∞—Ö—Å—Ç–∞–Ω—Å–∫–∏–π –û–§'
,'–†–§ –°–µ–º–µ–π'
,'–ê–∫–º–æ–ª–∏–Ω—Å–∫–∏–π –û–§'
,'–®—ã–º–∫–µ–Ω—Ç—Å–∫–∏–π –≥–æ—Ä–æ–¥—Å–∫–æ–π —Ñ–∏–ª–∏–∞–ª'
,'–ê—Å—Ç–∞–Ω–∏–Ω—Å–∫–∏–π –≥–æ—Ä–æ–¥—Å–∫–æ–π —Ñ–∏–ª–∏–∞–ª'
,'–ñ–∞–Ω–∞–æ–∑–µ–Ω—Å–∫–∏–π –†–§'
,'–ê–ª–º–∞—Ç–∏–Ω—Å–∫–∏–π –û–§'
,'–ñ–µ–∑–∫–∞–∑–≥–∞–Ω—Å–∫–∏–π –†–§'
,'–Ω–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–æ'
,'–≠–∫–∏–±–∞—Å—Ç—É–∑—Å–∫–∏–π –†–§'
,'–ö–æ—Å—Ç–∞–Ω–∞–π—Å–∫–∏–π –û–§'
,'–¢–µ–º–∏—Ä—Ç–∞—É—Å–∫–∏–π –†–§'
,'–ë–∞–π–∫–æ–Ω—ã—Ä—Å–∫–∏–π –†–§'
,'–ñ–∞–º–±—ã–ª—Å–∫–∏–π –û–§'
,'–ö–∞—Ä–∞–≥–∞–Ω–¥–∏–Ω—Å–∫–∏–π –û–§'
,'–ê–û –ù–∞—Ä–æ–¥–Ω—ã–π –ë–∞–Ω–∫ –†–µ—Å–ø—É–±–ª–∏–∫–∏ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω'
,'–®—ã–º–∫–µ–Ω—Ç—Å–∫–∏–π —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ñ–∏–ª–∏–∞–ª'
,'–ê–∫—Ç—é–±–∏–Ω—Å–∫–∏–π –û–§'
,'–ê–ª–º–∞—Ç–∏–Ω—Å–∫–∏–π –æ–±–ª–∞—Å—Ç–Ω–æ–π —Ñ–∏–ª–∏–∞–ª –≥.–ö–æ–Ω–∞–µ–≤'
,'–û–§ ¬´–ê–±–∞–π¬ª'
,'–ó–∞–ø–∞–¥–Ω–æ-–ö–∞–∑–∞—Ö—Å—Ç–∞–Ω—Å–∫–∏–π –û–§'
,'–û–§ ¬´“∞–ª—ã—Ç–∞—É¬ª'
,'–ë–∞–ª—Ö–∞—à—Å–∫–∏–π –†–§'
,'–¢–∞–ª–¥—ã–∫–æ—Ä–≥–∞–Ω—Å–∫–∏–π –û–§'
,'–û–§ ¬´–ñ–µ—Ç—ñ—Å—É¬ª'
,'–ê—Ç—ã—Ä–∞—É—Å–∫–∏–π –û–§'
,'–ú–∞–Ω–≥–∏—Å—Ç–∞—É—Å–∫–∏–π –û–§'
,'–ê—Å—Ç–∞–Ω–∏–Ω—Å–∫–∏–π –†–§'
,'–ê–ª–º–∞—Ç–∏–Ω—Å–∫–∏–π –ì–§'
,'–í–æ—Å—Ç–æ—á–Ω–æ-–ö–∞–∑–∞—Ö—Å—Ç–∞–Ω—Å–∫–∏–π –û–§'
,'–ö—ã–∑—ã–ª–æ—Ä–¥–∏–Ω—Å–∫–∏–π –û–§'
,'–ì–æ–ª–æ–≤–Ω–æ–π –ë–∞–Ω–∫'
,'–°–µ–º–∏–ø–∞–ª–∞—Ç–∏–Ω—Å–∫–∏–π –†–§'
]
            st.subheader('–§–∏–ª—å—Ç—Ä –¥–ª—è —Ñ–∏–ª–∏–∞–ª–æ–≤')
            selected_filials = st.multiselect("–í—ã–±–µ—Ä–∏–µ—Ç —Ñ–∏–ª–∏–∞–ª—ã –∫ –æ—á–∏—Å—Ç–∫–µ:", filial_list)

            if 'backup' not in st.session_state or st.session_state['backup'].empty:
                st.session_state['backup'] = pd.DataFrame()

            col1, col2 = st.columns(2)

            with col1:

                if st.button('Filter —Ñ–∏–ª–∏–∞–ª—ã'):

                    st.session_state['backup'] = st.session_state.df
                   
                    with get_connection_DSSB_OCDS() as conn:
                        df_filials = get_iin_bin_for_filials(conn, selected_filials)

                        iins_to_keep = set(df_filials['IIN'])
                   
                        # Filter df1 to keep only rows where IIN is in df2
                        st.session_state.df = st.session_state.df[st.session_state.df['IIN'].isin(iins_to_keep)]
                        st.write(st.session_state.df.head(500))
                        num_rows = st.session_state.df.shape[0]
                       
   
                   
                        st.success("–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.")
                        st.success(f"–í –±–∞–∑–µ {num_rows} –∑–∞–ø–∏—Å–µ–π")
                        #st.experimental_rerun()

            with col2:

                if st.button('–í–µ—Ä–Ω—É—Ç—å –±–∞–∑—É –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞ —Ñ–∏–ª–∏–∞–ª–æ–≤') and not st.session_state['backup'].empty:
                    st.session_state.df = st.session_state['backup']
                    st.success('–±–∞–∑–∞ –≤–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–∞ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞ —Ñ–∏–ª–∏–∞–ª–æ–≤')

                    st.write(st.session_state.df.head(500))
                    num_rows = st.session_state.df.shape[0]
                   

               
                    st.success("–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.")
                    st.success(f"–í –±–∞–∑–µ {num_rows} –∑–∞–ø–∏—Å–µ–π")
                    #st.experimental_rerun()
               
            st.write('-----------------------------------------------------------')
            st.subheader('–§–∏–ª—å—Ç—Ä –¥–ª—è –≤–æ–∑—Ä–∞—Å—Ç–∞ –∏ –ø–æ–ª–∞')
            st.subheader('–ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ 5 –º–∏–Ω')
            min_age = st.number_input('Enter minimum age', min_value=0, max_value=120, step=1, value=0)
            max_age = st.number_input('Enter maximum age', min_value=0, max_value=120, step=1, value=120)
            sex_selection = st.multiselect('Select sex type', options=['M', 'F'], default=['M', 'F'])

            col3, col4 = st.columns(2)

            with col3:
                if st.button('Filter DataFrame'):

                    st.session_state['backup'] = st.session_state.df
                   
                    with get_connection_DSSB_OCDS() as conn:
                        df2 = load_sql_table(conn)
                        st.write('got df')
                        filtered_df2 = filter_dataframe_extra(df2, min_age, max_age, sex_selection)
               
                        st.session_state.df = filter_df1_based_on_df2(st.session_state.df, filtered_df2)
                        st.write("Filtered df1 based on df2:")
                        st.write(st.session_state.df.head(500))
                        num_rows = st.session_state.df.shape[0]
       
                   
                        st.success(f"–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω—ã")
                        st.success(f"–í –±–∞–∑–µ {num_rows} –∑–∞–ø–∏—Å–µ–π –æ—á–∏—Å—Ç–∫–∏")
                        #st.experimental_rerun()
            with col4:
                if st.button('–í–µ—Ä–Ω—É—Ç—å –±–∞–∑—É –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞ –≤–æ–∑—Ä–∞—Å—Ç–∞ –∏ –ø–æ–ª–∞') and not st.session_state['backup'].empty:
                    st.session_state.df = st.session_state['backup']
                    st.success('–±–∞–∑–∞ –≤–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–∞ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞ –≤–æ–∑—Ä–∞—Å—Ç–∞ –∏ –ø–æ–ª–∞')

                    st.write(st.session_state.df.head(500))
                    num_rows = st.session_state.df.shape[0]
                   

               
                    st.success("–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.")
                    st.success(f"–í –±–∞–∑–µ {num_rows} –∑–∞–ø–∏—Å–µ–π")
                    #st.experimental_rerun()

            st.write('-----------------------------------------------------------')
            st.subheader('–û—á–∏—Å—Ç–∫–∞ —Å—É–º–º')
            user_input = st.number_input("–í—ã–±–µ—Ä–∏—Ç–µ —Å—É–º–º—É:")

            col5, col6 = st.columns(2)

            with col5:
                if st.button('–û—á–∏—Å—Ç–∏—Ç—å –≤—Å–µ—Ö —Å —Å—É–º–º–æ–π –º–µ–Ω—å—à–µ') and user_input:
                    st.session_state['backup'] = st.session_state.df
                    st.session_state.df = st.session_state.df[st.session_state.df['Column_sum'] >= user_input]
                    st.write(st.session_state.df.head(500))
                                               
                    num_rows = st.session_state.df.shape[0]
   
               
                    st.success(f"–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω—ã, –≤—Å–µ —Å —Å—É–º–º–æ–π –º–µ–Ω–µ—à–µ —á–µ–º {user_input} —É–¥–∞–ª–µ–Ω–Ω—ã")
                    st.success(f"–í –±–∞–∑–µ {num_rows} –∑–∞–ø–∏—Å–µ–π –ø–æ—Å–ª–µ –±–ª—ç–∫ –∏ —Å—Ç–æ–ø –ª–∏—Å—Ç–æ–≤")
                    #st.experimental_rerun()
            with col6:
                if st.button('–í–µ—Ä–Ω—É—Ç—å –±–∞–∑—É –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞ —Å—É–º–º—ã') and not st.session_state['backup'].empty:
                    st.session_state.df = st.session_state['backup']
                    st.success('–±–∞–∑–∞ –≤–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–∞ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞ —Å—É–º–º—ã')

                    st.write(st.session_state.df.head(500))
                    num_rows = st.session_state.df.shape[0]
                   

               
                    st.success("–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.")
                    st.success(f"–í –±–∞–∑–µ {num_rows} –∑–∞–ø–∏—Å–µ–π")
                    #st.experimental_rerun()

            st.write('-----------------------------------------------------------')
            st.subheader('–£–∫–æ—Ä–æ—Ç–∏—Ç—å –±–∞–∑—É')
            max_rows = st.number_input("–í—ã–±–µ—Ä–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏–Ω–∏–π:", min_value=1, value=20)
           
            # Button to trim the DataFrame
            if st.button('–£–∫–æ—Ä–æ—Ç–∏—Ç—å –≤—ã–±–æ—Ä–∫—É'):
                st.session_state.df = trim_dataframe(st.session_state.df, max_rows)
                num_rows = st.session_state.df.shape[0]
                st.write(f"Trimmed DataFrame (first {max_rows} rows)")
                st.success(f"–í –±–∞–∑–µ {num_rows} –∑–∞–ø–∏—Å–µ–π")
                st.write(st.session_state.df)
                #st.experimental_rerun()
               
               
   
        CAMPAIGNCODE_var = None
        stratify_check = None
        jira_check = None
       
        if not st.session_state.df.empty:
            stratify_check = st.toggle('–°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å DF?')
            jira_check = st.toggle('Jira')

        if jira_check:
            if 'df' not in st.session_state or st.session_state.df.empty:
                st.session_state.df = pd.DataFrame()
            jira_main()
            st.write('-----------------------------------------------------------')
       
        if stratify_check:
            # Initialize session state
            if 'open_new_tab' not in st.session_state:
                st.session_state.open_new_tab = False
           
            def set_open_new_tab():
                st.session_state.open_new_tab = True
           
            st.button("Stratification", on_click=set_open_new_tab)

            if st.button('–î–æ–±–∞–≤–∏—Ç—å –ü—Ä–æ–¥—É–∫—Ç—ã'):

                products = pd.read_parquet('Databases/dssb_app.products_per_fl_prod.parquet')
                st.write(products.head(500))

                st.session_state.df = pd.merge(st.session_state.df, products, on = 'IIN', how = 'inner')
           
            # Check if we should open a new tab
            if st.session_state.open_new_tab:
                js = """
                <script>
                window.open('http://172.28.80.18:8525/&#39;, '_blank');
                </script>
                """
                st.components.v1.html(js, height=0, width=0)
                st.session_state.open_new_tab = False
           
           
            buffer = io.BytesIO()
            table = pa.Table.from_pandas(st.session_state.df)
            pq.write_table(table, buffer, compression='snappy')
            buffer.seek(0)
            st.download_button(
                label=f"–°–∫–∞—á–∞—Ç—å –†–∞–∑–±–∏–µ–Ω–∏–µ –∫–∞–∫ Parquet",
                data=buffer,
                file_name=f'DF.parquet',
                mime='application/octet-stream',
            )



        #////////////////////////////////////////////////////////////////////////////////////////////////////////////////
        # Loading data into Table


        def display_upload_status():
            """Display current upload readiness status to users"""
            if 'df' not in st.session_state or st.session_state.df.empty:
                st.warning("‚ö†Ô∏è **–°—Ç–∞—Ç—É—Å –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏:** –î–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
                return False
            
            if 'saved_data' not in st.session_state or st.session_state.saved_data != 1:
                st.warning("‚ö†Ô∏è **–°—Ç–∞—Ç—É—Å –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏:** –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫–∞–º–ø–∞–Ω–∏–∏ –Ω–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
                return False
                
            missing_fields = []
            if not st.session_state.get('CAMPAIGNCODE'):
                missing_fields.append('CAMPAIGNCODE')
            if not st.session_state.get('STREAM'):
                missing_fields.append('STREAM')
            if not st.session_state.get('SHORT_DESC'):
                missing_fields.append('SHORT_DESC')
                
            if missing_fields:
                st.error(f"‚ùå **–°—Ç–∞—Ç—É—Å –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏:** –ù–µ –∑–∞–ø–æ–ª–Ω–µ–Ω—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è: {', '.join(missing_fields)}")
                return False
            else:
                st.success("‚úÖ **–°—Ç–∞—Ç—É—Å –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏:** –í—Å–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–π–¥–µ–Ω—ã, –≥–æ—Ç–æ–≤–æ –∫ –∑–∞–≥—Ä—É–∑–∫–µ")
                return True


        def safe_database_upload_with_skip(df, get_connection_func, table_name, column_mapping, additional_info, operation_name, allow_skip=True):
            """
            Safely perform database upload with ability to skip problematic records
            """
            try:
                # Pre-upload validation
                if df is None or df.empty:
                    st.error("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—ã–±–æ—Ä–∫—É –∫–ª–∏–µ–Ω—Ç–æ–≤.")
                    return False, []
                
                # Show what we're about to do
                with st.spinner(f'–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤ {table_name}...'):
                    # Get database connection
                    try:
                        conn = get_connection_func()
                    except Exception as conn_error:
                        st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö: –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑ –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É.")
                        logging.error(f"Connection error for {operation_name}: {conn_error}")
                        return False, []
                    
                    # Choose the appropriate upload function
                    if allow_skip and len(df) > 1:  # Use skip function for bulk uploads
                        success, message, records_count, failed_records = insert_dataframe_to_oracle_with_skip(
                            df, conn, table_name, column_mapping, additional_info
                        )
                    else:  # Use original function for single records or when skip is disabled
                        success, message, records_count = insert_dataframe_to_oracle(
                            df, conn, table_name, column_mapping, additional_info
                        )
                        failed_records = []
                    
                    # Display result to user
                    if success:
                        st.success(message)
                        
                        # Show detailed results if there were failed records
                        if failed_records:
                            with st.expander(f"‚ö†Ô∏è –ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏: {len(failed_records)} –∑–∞–ø–∏—Å–µ–π –ø—Ä–æ–ø—É—â–µ–Ω–æ", expanded=False):
                                st.write("**–ü—Ä–∏—á–∏–Ω—ã –ø—Ä–æ–ø—É—Å–∫–∞ –∑–∞–ø–∏—Å–µ–π:**")
                                
                                # Group by error type
                                error_groups = {}
                                for failed in failed_records:
                                    error_type = failed['error']
                                    if error_type not in error_groups:
                                        error_groups[error_type] = []
                                    error_groups[error_type].append(failed)
                                
                                for error_type, records in error_groups.items():
                                    st.write(f"- **{error_type}**: {len(records)} –∑–∞–ø–∏—Å–µ–π")
                                    if len(records) <= 5:  # Show details for small numbers
                                        for record in records:
                                            st.write(f"  - –°—Ç—Ä–æ–∫–∞ {record['row_index']}: {record['data']}")
                                    elif len(records) <= 20:
                                        st.write(f"  - –°—Ç—Ä–æ–∫–∏: {', '.join([str(r['row_index']) for r in records])}")
                                    else:
                                        st.write(f"  - –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏ (–≤—Å–µ–≥–æ {len(records)})")
                        
                        logging.info(f"Successful {operation_name}: {records_count} records loaded, {len(failed_records)} skipped")
                        return True, failed_records
                    else:
                        st.error(message)
                        if records_count > 0:
                            st.warning(f"‚ö†Ô∏è –ß–∞—Å—Ç–∏—á–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞: {records_count} –∑–∞–ø–∏—Å–µ–π –±—ã–ª–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–π –æ—à–∏–±–∫–∏.")
                        
                        # Show failed records if any
                        if failed_records:
                            with st.expander(f"‚ùå –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –∑–∞–ø–∏—Å–∏: {len(failed_records)}", expanded=True):
                                for failed in failed_records[:10]:  # Show first 10
                                    st.write(f"–°—Ç—Ä–æ–∫–∞ {failed['row_index']}: {failed['error']}")
                                if len(failed_records) > 10:
                                    st.write(f"... –∏ –µ—â–µ {len(failed_records) - 10} –∑–∞–ø–∏—Å–µ–π")
                        
                        logging.error(f"Failed {operation_name}: {message}")
                        return False, failed_records
                        
            except Exception as e:
                st.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≤ {table_name}: {str(e)[:200]}...")
                logging.error(f"Critical error in {operation_name}: {e}")
                return False, []


        def safe_database_upload(df, get_connection_func, table_name, column_mapping, additional_info, operation_name):
            """
            Safely perform database upload with comprehensive error handling and user feedback
            """
            try:
                # Pre-upload validation
                if df is None or df.empty:
                    st.error("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—ã–±–æ—Ä–∫—É –∫–ª–∏–µ–Ω—Ç–æ–≤.")
                    return False
                
                # Show what we're about to do
                with st.spinner(f'–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤ {table_name}...'):
                    # Get database connection
                    try:
                        conn = get_connection_func()
                    except Exception as conn_error:
                        st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö: –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑ –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É.")
                        logging.error(f"Connection error for {operation_name}: {conn_error}")
                        return False
                    
                    # Perform the upload
                    success, message, records_count = insert_dataframe_to_oracle(
                        df, conn, table_name, column_mapping, additional_info
                    )
                    
                    # Display result to user
                    if success:
                        st.success(message)
                        logging.info(f"Successful {operation_name}: {records_count} records")
                        return True
                    else:
                        st.error(message)
                        if records_count > 0:
                            st.warning(f"‚ö†Ô∏è –ß–∞—Å—Ç–∏—á–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞: {records_count} –∑–∞–ø–∏—Å–µ–π –±—ã–ª–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ –æ—à–∏–±–∫–∏.")
                        logging.error(f"Failed {operation_name}: {message}")
                        return False
                        
            except Exception as e:
                st.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≤ {table_name}: {str(e)[:200]}...")
                logging.error(f"Critical error in {operation_name}: {e}")
                return False


        def insert_dataframe_to_oracle(df, conn, table_name, column_mapping, additional_info, batch_size=10000):
            """
            Insert dataframe to Oracle database with comprehensive error handling
            Returns: (success: bool, error_message: str, records_inserted: int)
            """
            cursor = None
            records_inserted = 0
            
            try:
                # Pre-validation checks
                if df is None or df.empty:
                    return False, "‚ùå –û—à–∏–±–∫–∞: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—ã–±–æ—Ä–∫—É –∫–ª–∏–µ–Ω—Ç–æ–≤.", 0
                
                if not conn:
                    return False, "‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.", 0
                
                cursor = conn.cursor()
       
                # Create a set of all keys to ensure uniqueness
                all_keys = list(set(column_mapping.values()) | set(additional_info.keys()))
        
                # Construct the fields string dynamically, ensuring uniqueness
                fields = ', '.join(all_keys)
        
                # Prepare the insert query
                insert_query = f"INSERT INTO {table_name} ({fields}) VALUES ({', '.join([':' + str(i+1) for i in range(len(all_keys))])})"
        
                # Prepare data for insertion
                data_to_insert = []
                for _, row in df.iterrows():
                    row_data = []
                    for key in all_keys:
                        if key in column_mapping.values():
                            original_col = next(col for col, mapped_col in column_mapping.items() if mapped_col == key)
                            row_data.append(row[original_col] if original_col in df.columns else None)
                        else:
                            row_data.append(additional_info.get(key))
                    data_to_insert.append(row_data)
        
                total_records = len(data_to_insert)
                num_batches = (total_records + batch_size - 1) // batch_size
        
                # Initialize Streamlit progress bar
                progress_bar = st.progress(0)
                progress_text = st.empty()
       
                # Batch insert with progress bar
                for i in range(num_batches):
                    start_idx = i * batch_size
                    end_idx = min(start_idx + batch_size, total_records)
                    batch = data_to_insert[start_idx:end_idx]
                    
                    try:
                        cursor.executemany(insert_query, batch)
                        conn.commit()
                        records_inserted += len(batch)
       
                        # Update progress bar
                        progress = (end_idx / total_records)
                        progress_bar.progress(progress)
                        progress_text.text(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {records_inserted}/{total_records} –∑–∞–ø–∏—Å–µ–π... {int(progress * 100)}%")
       
                    except Exception as batch_error:
                        conn.rollback()
                        error_msg = str(batch_error).lower()
                        
                        # Handle specific Oracle errors with user-friendly messages
                        if "unique constraint" in error_msg or "ora-00001" in error_msg:
                            return False, f"‚ùå –û—à–∏–±–∫–∞ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è: –ù–∞–π–¥–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã –≤ –¥–∞–Ω–Ω—ã—Ö. –í–æ–∑–º–æ–∂–Ω–æ –∫–∞–º–ø–∞–Ω–∏—è —É–∂–µ –±—ã–ª–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Ä–∞–Ω–µ–µ. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ CAMPAIGNCODE –∏–ª–∏ –æ—á–∏—Å—Ç–∏—Ç–µ –¥—É–±–ª–∏–∫–∞—Ç—ã.", records_inserted
                        elif "foreign key constraint" in error_msg or "ora-02291" in error_msg:
                            return False, f"‚ùå –û—à–∏–±–∫–∞ —Å–≤—è–∑–∏ –¥–∞–Ω–Ω—ã—Ö: –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–∞–º–ø–∞–Ω–∏–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ CAMPAIGNCODE –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫–∞–º–ø–∞–Ω–∏–∏.", records_inserted
                        elif "insufficient privileges" in error_msg or "ora-01031" in error_msg:
                            return False, f"‚ùå –û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–∞–≤ –¥–ª—è –∑–∞–ø–∏—Å–∏ –≤ —Ç–∞–±–ª–∏—Ü—É {table_name}. –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É.", records_inserted
                        elif "tablespace" in error_msg or "ora-01654" in error_msg:
                            return False, f"‚ùå –û—à–∏–±–∫–∞ –º–µ—Å—Ç–∞: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –º–µ—Å—Ç–∞ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö. –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É.", records_inserted
                        elif "table or view does not exist" in error_msg or "ora-00942" in error_msg:
                            return False, f"‚ùå –û—à–∏–±–∫–∞ —Å—Ö–µ–º—ã: –¢–∞–±–ª–∏—Ü–∞ {table_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É.", records_inserted
                        elif "too many values" in error_msg or "not enough values" in error_msg:
                            return False, f"‚ùå –û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞–Ω–Ω—ã—Ö: –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–ª–æ–Ω–æ–∫ –¥–∞–Ω–Ω—ã—Ö. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≤—ã–±–æ—Ä–∫–∏.", records_inserted
                        else:
                            return False, f"‚ùå –û—à–∏–±–∫–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {str(batch_error)[:200]}... –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É –µ—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è.", records_inserted
           
                progress_bar.empty()
                progress_text.empty()
                
                return True, f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {records_inserted} –∑–∞–ø–∏—Å–µ–π –≤ {table_name}", records_inserted
                
            except Exception as e:
                if conn:
                    conn.rollback()
                    
                error_msg = str(e).lower()
                
                # Handle connection and general errors
                if "network" in error_msg or "connection" in error_msg or "ora-12170" in error_msg or "ora-12154" in error_msg:
                    return False, "‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ç–∏: –ü—Ä–æ–±–ª–µ–º–∞ —Å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.", records_inserted
                elif "timeout" in error_msg or "ora-01013" in error_msg:
                    return False, "‚ùå –¢–∞–π–º-–∞—É—Ç: –û–ø–µ—Ä–∞—Ü–∏—è –∑–∞–Ω—è–ª–∞ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏.", records_inserted
                elif "memory" in error_msg or "ora-04030" in error_msg:
                    return False, "‚ùå –û—à–∏–±–∫–∞ –ø–∞–º—è—Ç–∏: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏.", records_inserted
                else:
                    return False, f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)[:200]}... –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É.", records_inserted
                    
            finally:
                if cursor:
                    cursor.close()
                if conn:
                    conn.close()
                # Clean up progress indicators
                try:
                    progress_bar.empty()
                    progress_text.empty()
                except:
                    pass
                return False, f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)[:200]}... –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É.", records_inserted


        def insert_dataframe_to_oracle_with_skip(df, conn, table_name, column_mapping, additional_info, batch_size=10000):
            """
            Insert dataframe to Oracle database with ability to skip problematic records
            Returns: (success: bool, message: str, successful_records: int, failed_records: list)
            """
            cursor = None
            successful_records = 0
            failed_records = []
            total_records = len(df)
            
            try:
                # Pre-validation checks
                if df is None or df.empty:
                    return False, "‚ùå –û—à–∏–±–∫–∞: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—ã–±–æ—Ä–∫—É –∫–ª–∏–µ–Ω—Ç–æ–≤.", 0, []
                
                if not conn:
                    return False, "‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.", 0, []
                
                cursor = conn.cursor()
           
                # Create a set of all keys to ensure uniqueness
                all_keys = list(set(column_mapping.values()) | set(additional_info.keys()))
           
                # Construct the fields string dynamically, ensuring uniqueness
                fields = ', '.join(all_keys)
           
                # Prepare the insert query
                insert_query = f"INSERT INTO {table_name} ({fields}) VALUES ({', '.join([':' + str(i+1) for i in range(len(all_keys))])})"
           
                # Prepare data for insertion
                data_to_insert = []
                for idx, (_, row) in enumerate(df.iterrows()):
                    row_data = []
                    for key in all_keys:
                        if key in column_mapping.values():
                            original_col = next(col for col, mapped_col in column_mapping.items() if mapped_col == key)
                            row_data.append(row[original_col] if original_col in df.columns else None)
                        else:
                            row_data.append(additional_info.get(key))
                    data_to_insert.append((idx, row_data))
           
                # Initialize Streamlit progress indicators
                progress_bar = st.progress(0)
                progress_text = st.empty()
                status_text = st.empty()
           
                # Try batch insert first, fall back to individual records if needed
                def try_batch_insert(batch_data):
                    nonlocal successful_records, failed_records
                    
                    try:
                        # Try to insert the entire batch
                        batch_rows = [item[1] for item in batch_data]
                        cursor.executemany(insert_query, batch_rows)
                        conn.commit()
                        successful_records += len(batch_data)
                        return True
                    except Exception as batch_error:
                        # Batch failed, try individual records
                        conn.rollback()
                        error_msg = str(batch_error).lower()
                        
                        # If it's a uniqueness error, process records individually
                        if "unique" in error_msg or "ora-20001" in error_msg or "ora-00001" in error_msg:
                            for idx, row_data in batch_data:
                                try:
                                    cursor.execute(insert_query, row_data)
                                    conn.commit()
                                    successful_records += 1
                                except Exception as row_error:
                                    conn.rollback()
                                    row_error_msg = str(row_error)
                                    
                                    # Create readable error message
                                    if "unique" in row_error_msg.lower() or "ora-20001" in row_error_msg.lower():
                                        error_desc = "–î—É–±–ª–∏–∫–∞—Ç –∑–∞–ø–∏—Å–∏"
                                    elif "ora-02291" in row_error_msg.lower():
                                        error_desc = "–ù–∞—Ä—É—à–µ–Ω–∏–µ —Å–≤—è–∑–∏ –¥–∞–Ω–Ω—ã—Ö"
                                    else:
                                        error_desc = f"–û—à–∏–±–∫–∞ –¥–∞–Ω–Ω—ã—Ö: {row_error_msg[:100]}"
                                    
                                    failed_records.append({
                                        'row_index': idx,
                                        'error': error_desc,
                                        'data': row_data[:2] if len(row_data) >= 2 else row_data  # Show first 2 fields for identification
                                    })
                            return True
                        else:
                            # Different type of error, re-raise
                            raise batch_error
                
                # Process data in batches
                num_batches = (total_records + batch_size - 1) // batch_size
                
                for batch_idx in range(num_batches):
                    start_idx = batch_idx * batch_size
                    end_idx = min(start_idx + batch_size, total_records)
                    batch = data_to_insert[start_idx:end_idx]
                    
                    try_batch_insert(batch)
                    
                    # Update progress
                    progress = (end_idx / total_records)
                    progress_bar.progress(progress)
                    progress_text.text(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {end_idx}/{total_records} –∑–∞–ø–∏—Å–µ–π...")
                    
                    if failed_records:
                        status_text.text(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {successful_records} | ‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ: {len(failed_records)}")
                    else:
                        status_text.text(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {successful_records}")
           
                # Clean up progress indicators
                progress_bar.empty()
                progress_text.empty()
                status_text.empty()
                
                # Create result message
                if successful_records == total_records:
                    message = f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –≤—Å–µ {successful_records} –∑–∞–ø–∏—Å–µ–π –≤ {table_name}"
                elif successful_records > 0:
                    message = f"‚ö†Ô∏è –ß–∞—Å—Ç–∏—á–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞: {successful_records} –∏–∑ {total_records} –∑–∞–ø–∏—Å–µ–π –∑–∞–≥—Ä—É–∂–µ–Ω–æ –≤ {table_name}. –ü—Ä–æ–ø—É—â–µ–Ω–æ {len(failed_records)} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤/–ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π."
                else:
                    message = f"‚ùå –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å: –≤—Å–µ {total_records} –∑–∞–ø–∏—Å–µ–π –∏–º–µ—é—Ç –ø—Ä–æ–±–ª–µ–º—ã"
                
                return successful_records > 0, message, successful_records, failed_records
                
            except Exception as e:
                if conn:
                    conn.rollback()
                    
                error_msg = str(e).lower()
                
                # Handle connection and general errors
                if "network" in error_msg or "connection" in error_msg:
                    return False, "‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ç–∏: –ü—Ä–æ–±–ª–µ–º–∞ —Å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö.", successful_records, failed_records
                elif "timeout" in error_msg:
                    return False, "‚ùå –¢–∞–π–º-–∞—É—Ç: –û–ø–µ—Ä–∞—Ü–∏—è –∑–∞–Ω—è–ª–∞ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏.", successful_records, failed_records
                else:
                    return False, f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)[:200]}...", successful_records, failed_records
                    
            finally:
                if cursor:
                    cursor.close()
                if conn:
                    conn.close()
                # Clean up progress indicators
                try:
                    progress_bar.empty()
                    progress_text.empty()
                    status_text.empty()
                except:
                    pass


#/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
#/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
#/////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
       
        # Start of Streamlit UI
        if 'df' not in st.session_state or st.session_state.df.empty:
            st.warning("–î–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –≤—ã–±–µ—Ä–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ –∫ –∑–∞–≥—Ä—É–∑–∫–µ")
        else:
            table_name = 'dssb_ocds.mb01_camp_dict'
            campaign_id = st.text_input("Campaign ID:")

            if 'STREAM' not in st.session_state or st.session_state.df.empty:
                STREAM_var = None
            else:
                STREAM_var = st.session_state['STREAM']

            if 'SUB_STREAM' not in st.session_state or st.session_state.df.empty:
                SUB_STREAM_var = None
            else:
                SUB_STREAM_var = st.session_state['SUB_STREAM']


            if 'TARGET_ACTION' not in st.session_state or st.session_state.df.empty:
                TARGET_ACTION_var = None
            else:
                TARGET_ACTION_var = st.session_state['TARGET_ACTION']

            if 'CHANNEL' not in st.session_state or st.session_state.df.empty:
                CHANNEL_var = None
            else:
                CHANNEL_var = st.session_state['CHANNEL']

            if 'CAMPAIGN_TYPE' not in st.session_state or st.session_state.df.empty:
                CAMPAIGN_TYPE_var = None
            else:
                CAMPAIGN_TYPE_var = st.session_state['CAMPAIGN_TYPE']

            if 'CAMPAIGN_NAME' not in st.session_state or st.session_state.df.empty:
                CAMPAIGN_NAME_var = None
            else:
                CAMPAIGN_NAME_var = st.session_state['CAMPAIGN_NAME']

            if 'CAMPAIGN_DESC' not in st.session_state or st.session_state.df.empty:
                CAMPAIGN_DESC_var = None
            else:
                CAMPAIGN_DESC_var = st.session_state['CAMPAIGN_DESC']

            if 'CAMPAIGN_TEXT' not in st.session_state or st.session_state.df.empty:
                CAMPAIGN_TEXT_var = None
            else:
                CAMPAIGN_TEXT_var = st.session_state['CAMPAIGN_TEXT']

            if 'CAMPAIGN_MODEL' not in st.session_state or st.session_state.df.empty:
                CAMPAIGN_MODEL_var = None
            else:
                CAMPAIGN_MODEL_var = st.session_state['CAMPAIGN_MODEL']

            if 'CDS_LAUNCHER' not in st.session_state or st.session_state.df.empty:
                CDS_LAUNCHER_var = None
            else:
                CDS_LAUNCHER_var = st.session_state['CDS_LAUNCHER']

            if 'CAMPAIGN_TEXT_KZ' not in st.session_state or st.session_state.df.empty:
                CAMPAIGN_TEXT_KZ_var = None
            else:
                CAMPAIGN_TEXT_KZ_var = st.session_state['CAMPAIGN_TEXT_KZ']
            if 'CAMPAIGNCODE' not in st.session_state or st.session_state.df.empty:
                CAMPAIGNCODE_var = None
            else:
                CAMPAIGNCODE_var = st.session_state['CAMPAIGNCODE']

            if 'RECORD' not in st.session_state:
                st.session_state['RECORD'] = None

           
            if st.button('–°—É—â–µ—Å—Ç–≤—É—é—â–∞—è –∫–∞–º–ø–∞–Ω–∏—è'):
                if campaign_id:
                    with get_connection_DSSB_OCDS() as conn:
                        campaign_data = fetch_campaign_data_by_id_main(conn, campaign_id, 'dssb_ocds.mb01_camp_dict')
                        if campaign_data:
                            st.success("Campaign ID found. Filling up the rest of the fields...")

                            st.session_state['STREAM'] = campaign_data.get('STREAM')
                            st.session_state['SUB_STREAM'] = campaign_data.get('SUB_STREAM')
                            st.session_state['TARGET_ACTION'] = campaign_data.get('TARGET_ACTION')
                            st.session_state['CHANNEL'] = campaign_data.get('CHANNEL')
                            st.session_state['CAMPAIGN_TYPE'] = campaign_data.get('CAMPAIGN_TYPE')
                            st.session_state['CAMPAIGN_NAME'] = campaign_data.get('CAMPAIGN_NAME')
                            st.session_state['CAMPAIGN_DESC'] = campaign_data.get('CAMPAIGN_DESC')
                            st.session_state['CAMPAIGN_TEXT'] = campaign_data.get('CAMPAIGN_TEXT')
                            st.session_state['CAMPAIGN_MODEL'] = campaign_data.get('CAMPAIGN_MODEL')
                            st.session_state['CDS_LAUNCHER'] = campaign_data.get('CDS_LAUNCHER')
                            st.session_state['CAMPAIGN_TEXT_KZ'] = campaign_data.get('CAMPAIGN_TEXT_KZ')
                            st.session_state['CAMPAIGNCODE'] = campaign_data.get('CAMPAIGNCODE')
                           

                           

                            STREAM_var = campaign_data.get('STREAM')
                            SUB_STREAM_var = campaign_data.get('SUB_STREAM')
                            TARGET_ACTION_var = campaign_data.get('TARGET_ACTION')
                            CHANNEL_var = campaign_data.get('CHANNEL')
                            CAMPAIGN_TYPE_var = campaign_data.get('CAMPAIGN_TYPE')
                            CAMPAIGN_NAME_var = campaign_data.get('CAMPAIGN_NAME')
                            CAMPAIGN_DESC_var = campaign_data.get('CAMPAIGN_DESC')
                            CAMPAIGN_TEXT_var = campaign_data.get('CAMPAIGN_TEXT')
                            CAMPAIGN_MODEL_var = campaign_data.get('CAMPAIGN_MODEL')
                            CDS_LAUNCHER_var = campaign_data.get('CDS_LAUNCHER')
                            CAMPAIGN_TEXT_KZ_var = campaign_data.get('CAMPAIGN_TEXT_KZ')
                            CAMPAIGNCODE_var = campaign_data.get('CAMPAIGNCODE')
                       
           
                        else:
                            st.warning("Campaign ID not found. Please enter the details manually.")
                else:
                    st.error("Please enter a Campaign ID to check.")

            if st.button('–ù–æ–≤–∞—è –∫–æ–º–ø–∞–Ω–∏—è'):
                # Start of Streamlit UI
                if 'df' not in st.session_state or st.session_state.df.empty:
                    st.warning("–î–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –≤—ã–±–µ—Ä–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ –∫ –∑–∞–≥—Ä—É–∑–∫–µ")
                else:
                    table_name = 'msbmckinsey.URK_00035232_CDS_MSB'
                    with get_connection_DSSB_OCDS() as conn:
                        campaign_data = fetch_campaign_data_by_id(conn, 'dssb_ocds.mb01_camp_dict')
                        if campaign_data:
                            st.session_state['CAMPAIGNCODE'] = campaign_data.get('CAMPAIGNCODE')
                            CAMPAIGNCODE_old = campaign_data.get('CAMPAIGNCODE')
                            CAMPAIGNCODE_var = increment_string(CAMPAIGNCODE_old)
                            st.session_state.camp = CAMPAIGNCODE_var
                            st.session_state.saved_data = 0

            if 'CAMPAIGNCODE' not in st.session_state:
                st.warning('–í—ã–±–µ—Ä–∏—Ç–µ CAMPAIGNCODE')
            else:
                st.success(f'–ù–æ–≤—ã–π CAMPAIGNCODE: {st.session_state.camp}')


            if CAMPAIGNCODE_var is not None:

                if stratify_check:

                    stratification_path = st.selectbox("Stratification?:", ["Yes", "No"],  index = 1)
   
                    if stratification_path == 'No':
                        st.info('–ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫—É')
                    if stratification_path == 'Yes':
                        data_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –¥–∞–Ω–Ω—ã—Ö (Parquet)", type=['parquet'])
                        if data_file is not None:
                            st.session_state.df = pd.read_parquet(data_file)
                            st.write(st.session_state.df.head(500))
                            num_rows = st.session_state.df.shape[0]
                            st.success("–î–∞–Ω–Ω—ã–µ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã.")
                            st.success(f"–í —Å—Ç—Ä–∞—Ç–µ {num_rows} –∑–∞–ø–∏—Å–µ–π")


                col7, col8 = st.columns(2)
                       
                with col7:
                    channel_path = st.radio("–í—ã–±–µ—Ä–∏—Ç–µ channel:",('Push', 'POP-UP', '–°–ú–°'))

                with col8:
                    is_bonus = st.checkbox('–ë–æ–Ω—É—Å–Ω–∞—è –∫–∞–º–ø–∞–Ω–∏—è?')
                    if is_bonus:
                        if st.button('–ù–æ–≤—ã–π –∫–æ–¥ XLS_OW_ID'):
                            with get_connection_DSSB_OCDS() as conn:
                                campaign_data = fetch_campaign_data_by_XLS(conn)
                                if campaign_data:
                                    st.session_state['XLS_OW_ID'] = campaign_data.get('XLS_OW_ID')
                                    CAMPAIGNCODE_old = campaign_data.get('XLS_OW_ID')
                                    CAMPAIGNCODE_var = increment_string_XLS(CAMPAIGNCODE_old)
                                    st.success(f'–ù–æ–≤—ã–π XLS_OW_ID: {CAMPAIGNCODE_var}')

                RB_channel_path = st.radio("–í—ã–±–µ—Ä–∏—Ç–µ channel:",('RB1', 'RB3'))

               
                table_name_target = 'dssb_ocds.mb22_local_target'
                table_name_control = 'dssb_ocds.mb21_local_control'
                if channel_path == 'Push':
                    table_name_target_main = 'fd_rb2_campaigns_users'

                    table_name_off_limit = 'off_limit_campaigns_users'
                    st.success(f'–≥—Ä—É–∑–∏–º –≤ {table_name_target_main}')
                    st.success(f'–≥—Ä—É–∑–∏–º –≤ {table_name_off_limit}')
                if channel_path == 'POP-UP':
                    table_name_target_main = 'FD_RB2_POP_UP_CAMPAIGN'

                    table_name_off_limit = 'off_limit_campaigns_users'
                    st.success(f'–≥—Ä—É–∑–∏–º –≤ {table_name_target_main}')
                   
                if channel_path == '–°–ú–°':

                    table_name_target_main = 'fd_rb2_campaigns_users'

                    table_name_off_limit = 'off_limit_campaigns_users'
                    st.success(f'–≥—Ä—É–∑–∏–º –≤ {table_name_target_main}')
                    st.success(f'–≥—Ä—É–∑–∏–º –≤ {table_name_off_limit}')

                    if st.button('–î–æ–±–∞–≤–∏—Ç—å —Ç–µ–ª–µ—Ñ–æ–Ω'):
                        with get_connection_DSSB_OCDS() as conn:
                            phones = fetch_phones(conn)
                            st.session_state.df = pd.merge(st.session_state.df, phones, on = 'IIN', how = 'inner')

                if st.session_state.saved_data == 0:
                   
                    if RB_channel_path == 'RB1':
                        # Collecting campaign-related information
                        CAMPAIGNCODE = st.text_input("CAMPAIGNCODE:")
                        #CAMPAIGNCODE = st.text_input("Campaign ID:")
                        DATE_START = st.date_input("Start Date:")
                        DATE_END = st.date_input("End Date:")
                        STREAM = st.text_input("STREAM::", STREAM_var)
                        SUB_STREAM = st.text_input("SUB_STREAM:", SUB_STREAM_var)
                        TARGET_ACTION = st.text_input("TARGET_ACTION:", TARGET_ACTION_var)
                        CHANNEL = st.text_input("CHANNEL:", CHANNEL_var)
                        CAMPAIGN_TYPE = st.text_input("CAMPAIGN_TYPE:", CAMPAIGN_TYPE_var)
                        CAMPAIGN_NAME = st.text_input("CAMPAIGN_NAME:", CAMPAIGN_NAME_var)
                        CAMPAIGN_DESC = st.text_input("CAMPAIGN_DESC:", CAMPAIGN_DESC_var)
                        CAMPAIGN_TEXT = st.text_input("CAMPAIGN_TEXT:", CAMPAIGN_TEXT_var)
                        CAMPAIGN_MODEL = st.text_input("CAMPAIGN_MODEL:", CAMPAIGN_MODEL_var)
                        CDS_LAUNCHER = st.text_input("CDS_LAUNCHER:", CDS_LAUNCHER_var)
                        CAMPAIGN_TEXT_KZ = st.text_input("CAMPAIGN_TEXT_KZ:", CAMPAIGN_TEXT_KZ_var)
                        SHORT_DESC = st.text_input("SHORT_DESC:")

                        OUT_DATE = st.date_input("–¥–∞—Ç–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏, –æ–Ω–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –º–∏–Ω–∏–º—É–º –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –¥–µ–Ω—å:")
                        CAMP_CNT = st.text_input("–∫–æ–ª-–≤–æ –≤—ã–±–æ—Ä–∫–∏:")
   
                    if RB_channel_path == 'RB3':
                        # Collecting campaign-related information
                        CAMPAIGNCODE = st.text_input("CAMPAIGNCODE:")
                        #CAMPAIGNCODE = st.text_input("Campaign ID:")
                        DATE_START = st.date_input("Start Date:")
                        DATE_END = st.date_input("End Date:")
                        STREAM = st.text_input("STREAM::", STREAM_var)
                        SUB_STREAM = st.text_input("SUB_STREAM:", SUB_STREAM_var)
                        TARGET_ACTION = st.text_input("TARGET_ACTION:", TARGET_ACTION_var)
                        CHANNEL = st.text_input("CHANNEL:", CHANNEL_var)
                        CAMPAIGN_TYPE = st.text_input("CAMPAIGN_TYPE:", CAMPAIGN_TYPE_var)
                        CAMPAIGN_NAME = st.text_input("CAMPAIGN_NAME:", CAMPAIGN_NAME_var)
                        CAMPAIGN_DESC = st.text_input("CAMPAIGN_DESC:", CAMPAIGN_DESC_var)
                        CAMPAIGN_TEXT = st.text_input("CAMPAIGN_TEXT:", CAMPAIGN_TEXT_var)
                        CAMPAIGN_MODEL = st.text_input("CAMPAIGN_MODEL:", CAMPAIGN_MODEL_var)
                        CDS_LAUNCHER = st.text_input("CDS_LAUNCHER:", CDS_LAUNCHER_var)
                        CAMPAIGN_TEXT_KZ = st.text_input("CAMPAIGN_TEXT_KZ:", CAMPAIGN_TEXT_KZ_var)
                        SHORT_DESC = st.text_input("SHORT_DESC:")
                        BONUS = st.text_input("BONUS:")
                        CHARACTERISTIC_JSON = st.text_input("CHARACTERISTIC_JSON:")
                        XLS_OW_ID = st.text_input("XLS_OW_ID:")

                        OUT_DATE = st.date_input("–¥–∞—Ç–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏, –æ–Ω–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –º–∏–Ω–∏–º—É–º –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –¥–µ–Ω—å:")
                        CAMP_CNT = st.text_input("–∫–æ–ª-–≤–æ –≤—ã–±–æ—Ä–∫–∏:")

                if st.button(f'–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ'):
                    if RB_channel_path == 'RB1':
                        fields = {
                            'DATE_START': DATE_START,
                            'DATE_END': DATE_END,
                            'STREAM': STREAM,
                            'SUB_STREAM': SUB_STREAM,
                            'TARGET_ACTION': TARGET_ACTION,
                            'CHANNEL': CHANNEL,
                            'CAMPAIGN_TYPE': CAMPAIGN_TYPE,
                            'CAMPAIGN_NAME': CAMPAIGN_NAME,
                            'CAMPAIGN_DESC': CAMPAIGN_DESC,
                            'CAMPAIGN_TEXT': CAMPAIGN_TEXT,
                            'CAMPAIGN_MODEL': CAMPAIGN_MODEL,
                            'CDS_LAUNCHER': CDS_LAUNCHER,
                            'CAMPAIGN_TEXT_KZ': CAMPAIGN_TEXT_KZ,
                            'SHORT_DESC': SHORT_DESC,
                            'OUT_DATE': OUT_DATE,
                            'CAMP_CNT': CAMP_CNT,
                        }
                    if RB_channel_path == 'RB3':
                        fields = {
                            'DATE_START': DATE_START,
                            'DATE_END': DATE_END,
                            'STREAM': STREAM,
                            'SUB_STREAM': SUB_STREAM,
                            'TARGET_ACTION': TARGET_ACTION,
                            'CHANNEL': CHANNEL,
                            'CAMPAIGN_TYPE': CAMPAIGN_TYPE,
                            'CAMPAIGN_NAME': CAMPAIGN_NAME,
                            'CAMPAIGN_DESC': CAMPAIGN_DESC,
                            'CAMPAIGN_TEXT': CAMPAIGN_TEXT,
                            'CAMPAIGN_MODEL': CAMPAIGN_MODEL,
                            'CDS_LAUNCHER': CDS_LAUNCHER,
                            'CAMPAIGN_TEXT_KZ': CAMPAIGN_TEXT_KZ,
                            'SHORT_DESC': SHORT_DESC,
                            'BONUS': BONUS,
                            'CHARACTERISTIC_JSON': CHARACTERISTIC_JSON,
                            'XLS_OW_ID': XLS_OW_ID,
                            'OUT_DATE': OUT_DATE,
                            'CAMP_CNT': CAMP_CNT,
                        }
                    # Set session state for each field
                    for key, value in fields.items():
                        st.session_state[key] = value if value else None
                   
                    # For CAMPAIGNCODE, if you need it separately
                    if CAMPAIGNCODE:
                        st.session_state.CAMPAIGNCODE = CAMPAIGNCODE
                    else:
                        st.session_state.CAMPAIGNCODE = None
                   
                    st.session_state.saved_data = 1
                    st.rerun()

                if  st.session_state.saved_data == 1:
                    if RB_channel_path == 'RB1':
                        st.info(f'CAMPAIGNCODE: {st.session_state.CAMPAIGNCODE}')
                        st.info(f'DATE_START: {st.session_state.DATE_START}')
                        st.info(f'DATE_END: {st.session_state.DATE_END}')
                        st.info(f'STREAM: {st.session_state.STREAM}')
                        st.info(f'SUB_STREAM: {st.session_state.SUB_STREAM}')
                        st.info(f'TARGET_ACTION: {st.session_state.TARGET_ACTION}')
                        st.info(f'CHANNEL: {st.session_state.CHANNEL}')
                        st.info(f'CAMPAIGN_TYPE: {st.session_state.CAMPAIGN_TYPE}')
                        st.info(f'CAMPAIGN_NAME: {st.session_state.CAMPAIGN_NAME}')
                        st.info(f'CAMPAIGN_DESC: {st.session_state.CAMPAIGN_DESC}')
                        st.info(f'CAMPAIGN_TEXT: {st.session_state.CAMPAIGN_TEXT}')
                        st.info(f'CAMPAIGN_MODEL: {st.session_state.CAMPAIGN_MODEL}')
                        st.info(f'CDS_LAUNCHER: {st.session_state.CDS_LAUNCHER}')
                        st.info(f'CAMPAIGN_TEXT_KZ: {st.session_state.CAMPAIGN_TEXT_KZ}')
                        st.info(f'SHORT_DESC: {st.session_state.SHORT_DESC}')
                        st.info(f'OUT_DATE: {st.session_state.OUT_DATE}')
                        st.info(f'CAMP_CNT: {st.session_state.CAMP_CNT}')

                    if RB_channel_path == 'RB3':
                        st.info(f'CAMPAIGNCODE: {st.session_state.CAMPAIGNCODE}')
                        st.info(f'DATE_START: {st.session_state.DATE_START}')
                        st.info(f'DATE_END: {st.session_state.DATE_END}')
                        st.info(f'STREAM: {st.session_state.STREAM}')
                        st.info(f'SUB_STREAM: {st.session_state.SUB_STREAM}')
                        st.info(f'TARGET_ACTION: {st.session_state.TARGET_ACTION}')
                        st.info(f'CHANNEL: {st.session_state.CHANNEL}')
                        st.info(f'CAMPAIGN_TYPE: {st.session_state.CAMPAIGN_TYPE}')
                        st.info(f'CAMPAIGN_NAME: {st.session_state.CAMPAIGN_NAME}')
                        st.info(f'CAMPAIGN_DESC: {st.session_state.CAMPAIGN_DESC}')
                        st.info(f'CAMPAIGN_TEXT: {st.session_state.CAMPAIGN_TEXT}')
                        st.info(f'CAMPAIGN_MODEL: {st.session_state.CAMPAIGN_MODEL}')
                        st.info(f'CDS_LAUNCHER: {st.session_state.CDS_LAUNCHER}')
                        st.info(f'CAMPAIGN_TEXT_KZ: {st.session_state.CAMPAIGN_TEXT_KZ}')
                        st.info(f'SHORT_DESC: {st.session_state.SHORT_DESC}')
                        st.info(f'BONUS: {st.session_state.BONUS}')
                        st.info(f'CHARACTERISTIC_JSON: {st.session_state.CHARACTERISTIC_JSON}')
                        st.info(f'XLS_OW_ID: {st.session_state.XLS_OW_ID}')
                        st.info(f'OUT_DATE: {st.session_state.OUT_DATE}')
                        st.info(f'CAMP_CNT: {st.session_state.CAMP_CNT}')

                    if RB_channel_path == 'RB1':

                        additional_info_two = {
                            'CAMPAIGNCODE': st.session_state.CAMPAIGNCODE,
                            'DATE_START': st.session_state.DATE_START,
                            'DATE_END': st.session_state.DATE_END,
                            'STREAM': st.session_state.STREAM,
                            'INSET_DATETIME': now,
                        }
       
                        additional_info_three = {
                            'CAMPAIGNCODE': st.session_state.CAMPAIGNCODE,
                            'UPLOAD_DATE': st.session_state.DATE_START,
                            'SHORT_DESC': st.session_state.SHORT_DESC,
                        }
                   
                        additional_info = {
                            'CAMPAIGNCODE': st.session_state.CAMPAIGNCODE,
                            #'DATE_START': st.session_state.DATE_START,
                            #'DATE_END': st.session_state.DATE_END,
                            'STREAM': st.session_state.STREAM,
                            'INSERT_DATETIME': now,
                            'SUB_STREAM': st.session_state.SUB_STREAM,
                            'TARGET_ACTION': st.session_state.TARGET_ACTION,
                            'CHANNEL': st.session_state.CHANNEL ,
                            'CAMPAIGN_TYPE': st.session_state.CAMPAIGN_TYPE,
                            'CAMPAIGN_NAME': st.session_state.CAMPAIGN_NAME,
                            'CAMPAIGN_DESC': st.session_state.CAMPAIGN_DESC,
                            'CAMPAIGN_TEXT': st.session_state.CAMPAIGN_TEXT,
                            'CAMPAIGN_MODEL': st.session_state.CAMPAIGN_MODEL,
                            'CDS_LAUNCHER': st.session_state.CDS_LAUNCHER,
                            'CAMPAIGN_TEXT_KZ': st.session_state.CAMPAIGN_TEXT_KZ,
                            'OUT_DATE': st.session_state.OUT_DATE,
                            'CAMP_CNT': st.session_state.CAMP_CNT,
                        }

                    if RB_channel_path == 'RB3':

                        additional_info_rb3 = {
                            'CAMPAIGNCODE': st.session_state.CAMPAIGNCODE,
                            'DATE_START': st.session_state.DATE_START,
                            'DATE_END': st.session_state.DATE_END,
                            'XLS_OW_ID': st.session_state.XLS_OW_ID,
                            'TARGET_ACTION': st.session_state.TARGET_ACTION,
                            'BONUS': st.session_state.BONUS,
                            'CHARACTERISTIC_JSON': st.session_state.CHARACTERISTIC_JSON,
                        }
   
                        additional_info_three = {
                            'CAMPAIGNCODE': st.session_state.CAMPAIGNCODE,
                            'UPLOAD_DATE': st.session_state.DATE_START,
                            'SHORT_DESC': st.session_state.SHORT_DESC,
                        }

                        additional_info_two = {
                            'CAMPAIGNCODE': st.session_state.CAMPAIGNCODE,
                            'DATE_START': st.session_state.DATE_START,
                            'DATE_END': st.session_state.DATE_END,
                            'STREAM': st.session_state.STREAM,
                            'INSET_DATETIME': now,
                        }
       
                        additional_info_three = {
                            'CAMPAIGNCODE': st.session_state.CAMPAIGNCODE,
                            'UPLOAD_DATE': st.session_state.DATE_START,
                            'SHORT_DESC': st.session_state.SHORT_DESC,
                        }
                   
                        additional_info = {
                            'CAMPAIGNCODE': st.session_state.CAMPAIGNCODE,
                            #'DATE_START': st.session_state.DATE_START,
                            #'DATE_END': st.session_state.DATE_END,
                            'STREAM': st.session_state.STREAM,
                            'INSERT_DATETIME': now,
                            'SUB_STREAM': st.session_state.SUB_STREAM,
                            'TARGET_ACTION': st.session_state.TARGET_ACTION,
                            'CHANNEL': st.session_state.CHANNEL ,
                            'CAMPAIGN_TYPE': st.session_state.CAMPAIGN_TYPE,
                            'CAMPAIGN_NAME': st.session_state.CAMPAIGN_NAME,
                            'CAMPAIGN_DESC': st.session_state.CAMPAIGN_DESC,
                            'CAMPAIGN_TEXT': st.session_state.CAMPAIGN_TEXT,
                            'CAMPAIGN_MODEL': st.session_state.CAMPAIGN_MODEL,
                            'CDS_LAUNCHER': st.session_state.CDS_LAUNCHER,
                            'CAMPAIGN_TEXT_KZ': st.session_state.CAMPAIGN_TEXT_KZ,
                            'OUT_DATE': st.session_state.OUT_DATE,
                            'CAMP_CNT': st.session_state.CAMP_CNT,


                           
                        }


                if  st.session_state.saved_data == 1:
                    with st.expander("–î–æ–±–∞–≤–∏—Ç—å –ò–ò–ù"):
                        extra_iin = st.text_input("IIN:")
                        if len(str(extra_iin)) != 12:
                            st.error('Please enter correct IIN')
                        else:
                            df_one_line = st.session_state.df.iloc[[0]]
                            df_one_line.at[0, 'IIN'] = extra_iin
                           
                            if st.button(f'–î–æ–±–∞–≤–∏—Ç—å –ò–ò–ù –≤ {table_name_target} –∏ {table_name_target_main}'):
                                if st.session_state.CAMPAIGNCODE:  # Check mandatory fields
                                    column_mapping = {
                                        'IIN': 'IIN',
                                        'IIN': 'P_SID',
                                    }
                                   
                                    # Add IIN to mb22_local_target
                                    conn_dssb = get_connection_DSSB_OCDS()
                                    insert_dataframe_to_oracle(df_one_line, conn_dssb, table_name_target, column_mapping, additional_info_two)
                                    st.success("–ò–ò–ù —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω –≤ mb22_local_target")
                                   
                                    # Add IIN to fd_rb2_campaigns_users
                                    conn_spss = get_connection_SPSS_OCDS()
                                    insert_dataframe_to_oracle(df_one_line, conn_spss, table_name_target_main, column_mapping, additional_info_three)
                                    st.success("–ò–ò–ù —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω –≤ fd_rb2_campaigns_users")
                                   
                                    # Log the addition of IIN
                                    logging.info(f"IIN {extra_iin} added to both tables - User: {st.session_state.get('username', 'Unknown')}, Campaign Code: {st.session_state.CAMPAIGNCODE}")
                                else:
                                    st.error("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –∑–∞–ø–æ–ª–Ω–∏—Ç–µ –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–æ–ª—è")
                                    logging.warning(f"Failed to add IIN {extra_iin} - Missing fields - User: {st.session_state.get('username', 'Unknown')}, Campaign Code: {st.session_state.CAMPAIGNCODE}")
           
   
                    col10, col11 = st.columns(2)
                    # Separate buttons in an expander
   
                    # Display upload readiness status
                    st.subheader("–ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫ –∑–∞–≥—Ä—É–∑–∫–µ")
                    display_upload_status()
                    
                    # Skip mode toggle
                    skip_mode = st.checkbox(
                        "üîÑ **–†–µ–∂–∏–º –ø—Ä–æ–ø—É—Å–∫–∞ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π**", 
                        value=True, 
                        help="–í–∫–ª—é—á–∏—Ç–µ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ–ø—É—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π. –í—ã–∫–ª—é—á–∏—Ç–µ –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø—Ä–∏ –ø–µ—Ä–≤–æ–π –æ—à–∏–±–∫–µ."
                    )
                    
                    if skip_mode:
                        st.info("üí° **–†–µ–∂–∏–º –ø—Ä–æ–ø—É—Å–∫–∞ –∞–∫—Ç–∏–≤–µ–Ω**: –î—É–±–ª–∏–∫–∞—Ç—ã –∏ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –∑–∞–ø–∏—Å–∏ –±—É–¥—É—Ç –ø—Ä–æ–ø—É—â–µ–Ω—ã, –æ—Å—Ç–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∑—è—Ç—Å—è —É—Å–ø–µ—à–Ω–æ.")
                    else:
                        st.warning("‚ö†Ô∏è **–°—Ç—Ä–æ–≥–∏–π —Ä–µ–∂–∏–º**: –ó–∞–≥—Ä—É–∑–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –ø—Ä–∏ –ø–µ—Ä–≤–æ–π –æ—à–∏–±–∫–µ.")
                    
                    st.markdown("---")
   
                    num_rows = st.session_state.df.shape[0]

                    with col11:
                        # Button for mb01_camp_dict
                        if st.button(f'–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ mb01_camp_dict'):
                            table_name = 'dssb_ocds.mb01_camp_dict'
                            current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                            log_message = (f"User: {st.session_state['username']}, "
                                           f"Campaign Code: {st.session_state.CAMPAIGNCODE}, "
                                           f"Button Press Time: {current_time}, "
                                           f"STREAM: {st.session_state.STREAM}, "
                                           f"SHORT_DESC: {st.session_state.SHORT_DESC}, "
                                           f"Action: Upload to mb01_camp_dict")
                            logging.info(log_message)
                   
                            # Pre-validation
                            if not st.session_state.CAMPAIGNCODE:
                                st.error("‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: CAMPAIGNCODE –Ω–µ –∑–∞–ø–æ–ª–Ω–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –∑–∞–ø–æ–ª–Ω–∏—Ç–µ –∫–æ–¥ –∫–∞–º–ø–∞–Ω–∏–∏.")
                                logging.warning(f"Validation failed - Missing CAMPAIGNCODE - {log_message}")
                            elif not st.session_state.STREAM:
                                st.error("‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: STREAM –Ω–µ –∑–∞–ø–æ–ª–Ω–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –∑–∞–ø–æ–ª–Ω–∏—Ç–µ –ø–æ—Ç–æ–∫ –∫–∞–º–ø–∞–Ω–∏–∏.")
                                logging.warning(f"Validation failed - Missing STREAM - {log_message}")
                            else:
                                # Perform upload with improved error handling
                                column_mapping = {
                                    #'IIN': 'IIN',
                                    #'P_SID': 'P_SID',
                                }
                                df_one_line = st.session_state.df.iloc[[0]]
                                
                                success = safe_database_upload(
                                    df_one_line, 
                                    get_connection_DSSB_OCDS, 
                                    table_name, 
                                    column_mapping, 
                                    additional_info,
                                    f"Upload to {table_name}"
                                )
                                
                            if success:
                                logging.info(f"Successful upload to mb01_camp_dict - {log_message}")
                                #s = ['NadirB@halykbank.kz', 'MadiB@halykbank.kz', 'BibarysM@halykbank.kz', 'DaniyarMussa@halykbank.kz', 'AlfiyaEl@halykbank.kz']
                                s = ['NadirB@halykbank.kz']
                                email_message = f"Successful upload to mb01_camp_dict - {log_message}"
                                send_email(s, 'Automated campaign', email_message)
                            else:
                                logging.error(f"Failed upload to mb01_camp_dict - {log_message}")
   
                        st.write('')
   
                    with col10:
   
                        st.write('–¢–∞–±–ª–∏—Ü—ã –†–ë1:')
                        st.write('---------------------------------------------------------------------------------------------------------------------------------------------')
   
   
   
                    with col11:
   
   
                        # Button for rb3_tr_campaign_dict
                        if st.button(f'–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ rb3_tr_campaign_dict'):
                            table_name = 'dssb_ocds.rb3_tr_campaign_dict'
                            current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                            log_message = (f"User: {st.session_state['username']}, "
                                           f"Campaign Code: {st.session_state.CAMPAIGNCODE}, "
                                           f"Button Press Time: {current_time}, "
                                           f"STREAM: {st.session_state.STREAM}, "
                                           f"SHORT_DESC: {st.session_state.SHORT_DESC}, "
                                           f"Action: Upload to rb3_tr_campaign_dict")
                            logging.info(log_message)
                   
                            # Pre-validation
                            if not st.session_state.CAMPAIGNCODE:
                                st.error("‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: CAMPAIGNCODE –Ω–µ –∑–∞–ø–æ–ª–Ω–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –∑–∞–ø–æ–ª–Ω–∏—Ç–µ –∫–æ–¥ –∫–∞–º–ø–∞–Ω–∏–∏.")
                                logging.warning(f"Validation failed - Missing CAMPAIGNCODE - {log_message}")
                            elif not st.session_state.STREAM:
                                st.error("‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: STREAM –Ω–µ –∑–∞–ø–æ–ª–Ω–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –∑–∞–ø–æ–ª–Ω–∏—Ç–µ –ø–æ—Ç–æ–∫ –∫–∞–º–ø–∞–Ω–∏–∏.")
                                logging.warning(f"Validation failed - Missing STREAM - {log_message}")
                            else:
                                # Perform upload with improved error handling
                                column_mapping = {
                                    #'IIN': 'IIN',
                                    #'P_SID': 'P_SID',
                                }
                                df_one_line = st.session_state.df.iloc[[0]]
                                
                                success = safe_database_upload(
                                    df_one_line, 
                                    get_connection_DSSB_OCDS, 
                                    table_name, 
                                    column_mapping, 
                                    additional_info_rb3,
                                    f"Upload to {table_name}"
                                )
                                
                            if success:
                                logging.info(f"Successful upload to rb3_tr_campaign_dict - {log_message}")
                                #s = ['NadirB@halykbank.kz', 'MadiB@halykbank.kz', 'BibarysM@halykbank.kz', 'DaniyarMussa@halykbank.kz', 'AlfiyaEl@halykbank.kz']
                                s = ['NadirB@halykbank.kz']
                                email_message = f"Successful upload to rb3_tr_campaign_dict - {log_message}"
                                send_email(s, 'Automated campaign', email_message)
                            else:
                                    logging.error(f"Failed upload to rb3_tr_campaign_dict - {log_message}")
       
       
                        # Button for off_limit_campaigns_users
                        if st.button(f'–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ {table_name_off_limit}'):
                            current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                            log_message = (f"User: {st.session_state['username']}, "
                                           f"Campaign Code: {st.session_state.CAMPAIGNCODE}, "
                                           f"Button Press Time: {current_time}, "
                                           f"STREAM: {st.session_state.STREAM}, "
                                           f"ROW NUM: {num_rows}, "
                                           f"SHORT_DESC: {st.session_state.SHORT_DESC}, "
                                           f"Action: Upload to off_limit_campaigns_users")
                            logging.info(log_message)
                   
                            # Pre-validation
                            if not st.session_state.CAMPAIGNCODE:
                                st.error("‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: CAMPAIGNCODE –Ω–µ –∑–∞–ø–æ–ª–Ω–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –∑–∞–ø–æ–ª–Ω–∏—Ç–µ –∫–æ–¥ –∫–∞–º–ø–∞–Ω–∏–∏.")
                                logging.warning(f"Validation failed - Missing CAMPAIGNCODE - {log_message}")
                            else:
                                # Prepare column mapping based on channel
                                if channel_path != '–°–ú–°':
                                    column_mapping = {
                                        'IIN': 'IIN',
                                        'P_SID': 'P_SID',
                                    }
                                else:
                                    column_mapping = {
                                        'IIN': 'IIN',
                                        'P_SID': 'P_SID',
                                        'PHONE': 'PHONE',
                                    }
                                    # Check if phone column exists for SMS campaigns
                                    if 'PHONE' not in st.session_state.df.columns:
                                        st.error("‚ùå –û—à–∏–±–∫–∞ –¥–∞–Ω–Ω—ã—Ö: –î–ª—è –°–ú–° –∫–∞–º–ø–∞–Ω–∏–π —Ç—Ä–µ–±—É–µ—Ç—Å—è –∫–æ–ª–æ–Ω–∫–∞ PHONE. –ù–∞–∂–º–∏—Ç–µ '–î–æ–±–∞–≤–∏—Ç—å —Ç–µ–ª–µ—Ñ–æ–Ω' —Å–Ω–∞—á–∞–ª–∞.")
                                        logging.error(f"Missing PHONE column for SMS campaign - {log_message}")
                                        return
                                
                                                                # Perform upload with optional skip functionality  
                                if skip_mode:
                                    success, failed_records = safe_database_upload_with_skip(
                                        st.session_state.df, 
                                        get_connection_SPSS_OCDS, 
                                        table_name_off_limit, 
                                        column_mapping, 
                                        additional_info_three,
                                        f"Upload to {table_name_off_limit}",
                                        allow_skip=True
                                    )
                                else:
                                    success = safe_database_upload(
                                        st.session_state.df, 
                                        get_connection_SPSS_OCDS, 
                                        table_name_off_limit, 
                                        column_mapping, 
                                        additional_info_three,
                                        f"Upload to {table_name_off_limit}"
                                    )
                                    failed_records = []
                                
                                if success:
                                    if failed_records:
                                        logging.info(f"Partial upload to off_limit_campaigns_users - {log_message} - {len(failed_records)} records skipped")
                                        st.info(f"‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —Å –ø—Ä–æ–ø—É—Å–∫–æ–º {len(failed_records)} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
                                    else:
                                        logging.info(f"Successful upload to off_limit_campaigns_users - {log_message}")
                                else:
                                    logging.error(f"Failed upload to off_limit_campaigns_users - {log_message}")
   
                        st.write('')
   
                    with col10:
   
                        st.write('–¢–∞–±–ª–∏—Ü—ã –†–ë3:')
                        st.write('---------------------------------------------------------------------------------------------------------------------------------------------')
        
                    with col11:

                        # Button for fd_rb2_campaigns_users
                        if st.button(f'–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ {table_name_target_main}'):
                            current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                            log_message = (f"User: {st.session_state['username']}, "
                                           f"Campaign Code: {st.session_state.CAMPAIGNCODE}, "
                                           f"Button Press Time: {current_time}, "
                                           f"STREAM: {st.session_state.STREAM}, "
                                           f"ROW NUM: {num_rows}, "
                                           f"SHORT_DESC: {st.session_state.SHORT_DESC}, "
                                           f"Action: Upload to fd_rb2_campaigns_users")
                            logging.info(log_message)
                           
                            st.session_state['RECORD'] = st.session_state.CAMPAIGNCODE
                           
                            # Pre-validation
                            if not st.session_state.CAMPAIGNCODE:
                                st.error("‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: CAMPAIGNCODE –Ω–µ –∑–∞–ø–æ–ª–Ω–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –∑–∞–ø–æ–ª–Ω–∏—Ç–µ –∫–æ–¥ –∫–∞–º–ø–∞–Ω–∏–∏.")
                                logging.warning(f"Validation failed - Missing CAMPAIGNCODE - {log_message}")
                            elif not st.session_state.SHORT_DESC:
                                st.error("‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: SHORT_DESC –Ω–µ –∑–∞–ø–æ–ª–Ω–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –∑–∞–ø–æ–ª–Ω–∏—Ç–µ –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∫–∞–º–ø–∞–Ω–∏–∏.")
                                logging.warning(f"Validation failed - Missing SHORT_DESC - {log_message}")
                            else:
                                # Prepare data with IIN formatting
                                df_to_load = st.session_state.df.copy()
                               
                                # Create two columns from the same IIN data but with different types
                                if 'IIN' in df_to_load.columns:
                                    # IIN_STR will be used for the IIN column (as string with leading zeros)
                                    df_to_load['IIN_STR'] = df_to_load['IIN'].astype(str).str.zfill(12)  # Ensure 12 digits with leading zeros
                                   
                                    # IIN_INT will be used for the P_SID column (as integer without leading zeros)
                                    df_to_load['IIN_INT'] = df_to_load['IIN'].astype(int)
                                   
                                    # Then update the mapping to use these new columns
                                    column_mapping = {
                                        'IIN_STR': 'IIN',  # Map string version to IIN column
                                        'IIN_INT': 'P_SID'  # Map integer version to P_SID column
                                    }
                                else:
                                    st.error("‚ùå –û—à–∏–±–∫–∞ –¥–∞–Ω–Ω—ã—Ö: –í –≤—ã–±–æ—Ä–∫–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ IIN. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö.")
                                    logging.error(f"Missing IIN column in dataframe - {log_message}")
                                    return
                               
                                                                # Perform upload with optional skip functionality
                                if skip_mode:
                                    success, failed_records = safe_database_upload_with_skip(
                                        df_to_load, 
                                        get_connection_SPSS_OCDS, 
                                        table_name_target_main, 
                                        column_mapping, 
                                        additional_info_three,
                                        f"Upload to {table_name_target_main}",
                                        allow_skip=True
                                    )
                                else:
                                    success = safe_database_upload(
                                        df_to_load, 
                                        get_connection_SPSS_OCDS, 
                                        table_name_target_main, 
                                        column_mapping, 
                                        additional_info_three,
                                        f"Upload to {table_name_target_main}"
                                    )
                                    failed_records = []
                                
                                if success:
                                    if failed_records:
                                        logging.info(f"Partial upload to fd_rb2_campaigns_users - {log_message} - {len(failed_records)} records skipped")
                                        st.info(f"‚úÖ –ö–∞–º–ø–∞–Ω–∏—è {st.session_state.CAMPAIGNCODE} –≥–æ—Ç–æ–≤–∞ –∫ –∑–∞–ø—É—Å–∫—É! (–ü—Ä–æ–ø—É—â–µ–Ω–æ {len(failed_records)} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤)")
                                    else:
                                        logging.info(f"Successful upload to fd_rb2_campaigns_users - {log_message}")
                                        st.info(f"‚úÖ –ö–∞–º–ø–∞–Ω–∏—è {st.session_state.CAMPAIGNCODE} –≥–æ—Ç–æ–≤–∞ –∫ –∑–∞–ø—É—Å–∫—É!")
                                else:
                                    logging.error(f"Failed upload to fd_rb2_campaigns_users - {log_message}")
                                    st.warning("‚ö†Ô∏è –í–æ–∑–º–æ–∂–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã —Å –∑–∞–ø—É—Å–∫–æ–º –∫–∞–º–ø–∞–Ω–∏–∏ –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏.")
                   
                        # Button for mb22_local_target
                        if st.button(f'–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ {table_name_target}'):
                            current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                            log_message = (f"User: {st.session_state['username']}, "
                                           f"Campaign Code: {st.session_state.CAMPAIGNCODE}, "
                                           f"Button Press Time: {current_time}, "
                                           f"STREAM: {st.session_state.STREAM}, "
                                           f"ROW NUM: {num_rows}, "
                                           f"SHORT_DESC: {st.session_state.SHORT_DESC}, "
                                           f"Action: Upload to mb22_local_target")
                            logging.info(log_message)
                   
                            # Pre-validation
                            if not st.session_state.CAMPAIGNCODE:
                                st.error("‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: CAMPAIGNCODE –Ω–µ –∑–∞–ø–æ–ª–Ω–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –∑–∞–ø–æ–ª–Ω–∏—Ç–µ –∫–æ–¥ –∫–∞–º–ø–∞–Ω–∏–∏.")
                                logging.warning(f"Validation failed - Missing CAMPAIGNCODE - {log_message}")
                            else:
                                column_mapping = {
                                    'IIN': 'IIN',
                                    'IIN': 'P_SID',
                                }
                                
                                # Perform upload with improved error handling
                                success = safe_database_upload(
                                    st.session_state.df, 
                                    get_connection_DSSB_OCDS, 
                                    table_name_target, 
                                    column_mapping, 
                                    additional_info_two,
                                    f"Upload to {table_name_target}"
                                )
                                
                            if success:
                                logging.info(f"Successful upload to mb22_local_target - {log_message}")
                            else:
                                    logging.error(f"Failed upload to mb22_local_target - {log_message}")
   
                        st.write('')
   
                    with col10:
   
                        st.write('–¢–∞–±–ª–∏—Ü—ã –æ–±—â–∏–µ:')
                        st.write('---------------------------------------------------------------------------------------------------------------------------------------------')
   
                    if stratify_check:
                        with col11:
                            # Button for mb22_local_control
                            if st.button(f'–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ {table_name_control}'):
                                current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                                log_message = (f"User: {st.session_state['username']}, "
                                               f"Campaign Code: {st.session_state.CAMPAIGNCODE}, "
                                               f"Button Press Time: {current_time}, "
                                               f"STREAM: {st.session_state.STREAM}, "
                                               f"ROW NUM: {num_rows}, "
                                               f"SHORT_DESC: {st.session_state.SHORT_DESC}, "
                                               f"Action: Upload to mb22_local_control")
                                logging.info(log_message)
                       
                                # Pre-validation
                                if not st.session_state.CAMPAIGNCODE:
                                    st.error("‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: CAMPAIGNCODE –Ω–µ –∑–∞–ø–æ–ª–Ω–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –∑–∞–ø–æ–ª–Ω–∏—Ç–µ –∫–æ–¥ –∫–∞–º–ø–∞–Ω–∏–∏.")
                                    logging.warning(f"Validation failed - Missing CAMPAIGNCODE - {log_message}")
                                else:
                                    column_mapping = {
                                        'IIN': 'IIN',
                                        'IIN': 'P_SID',
                                    }
                                    
                                    # Perform upload with improved error handling
                                    success = safe_database_upload(
                                        st.session_state.df, 
                                        get_connection_DSSB_OCDS, 
                                        table_name_control, 
                                        column_mapping, 
                                        additional_info_two,
                                        f"Upload to {table_name_control}"
                                    )
                                    
                                if success:
                                    logging.info(f"Successful upload to mb22_local_control - {log_message}")
                                else:
                                        logging.error(f"Failed upload to mb22_local_control - {log_message}")
   
                            st.write('')
   
                        with col10:
   
                            st.write('–¢–∞–±–ª–∏—Ü—ã control:')
                            st.write('---------------------------------------------------------------------------------------------------------------------------------------------')
                           

               
                password_for_campaign = 'RBadmin123'
                if st.session_state['RECORD'] is not None:

                    if st.button("–°–∫–æ–ª—å–∫–æ –≤ –±–∞–∑–µ?"):
                        code = st.session_state['RECORD']
                        st.info(f'the code is {code}')
                        with get_connection_SPSS_OCDS() as conn:
                       
                            cursor = conn.cursor()
                           
                            # Execute the query
                            query = f"SELECT COUNT(IIN) FROM {table_name_target_main} WHERE CAMPAIGNCODE = '{code}'"
   
                           
                            cursor.execute(query)
                           
                            # Fetch the result
                            result = cursor.fetchone()
                            count = result[0] if result else 0
                           
                            # Display the result
                            st.info(f"–ù–∞ –∫–æ–¥: '{code}'. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {count}")

                    check_password = st.text_input("password")

                    if check_password == password_for_campaign:

                        if st.toggle('delete campaign code from db'):
                            code = st.session_state['RECORD']
                            st.write(f'–£–¥–∞–ª–∏—Ç—å –∫–∞–º–ø–∞–Ω–∏—é: {code}?')
                            if st.button("–£–¥–∞–ª–∏—Ç—å –∏–∑ –±–∞–∑—ã"):
                                code = st.session_state['RECORD']
                                st.info(f'–£–¥–∞–ª—è–µ–º {code}')
                                with get_connection_SPSS_OCDS() as conn:
                               
                                    cursor = conn.cursor()
                                   
                                    # Execute the query
                                    query = f"DELETE FROM {table_name_target_main} WHERE CAMPAIGNCODE = '{code}'"

                                    cursor.execute(query)
                                    conn.commit()                              
                                    # Display the result
                                    st.info(f"–ö–∞–º–ø–∞–Ω–∏—è: '{code}'. –£–¥–∞–ª–µ–Ω–∞ –∏–∑ –±–∞–∑—ã {table_name_target_main} —É—Å–ø–µ—à–Ω–æ.")
                           
                   
                   
       
                if st.button('Save to Excel'):
                    # Save the DataFrame to an Excel file
                    st.session_state.df.to_excel('MSB_CAMPAIGN.xlsx', index=False)
                   
                    # Open and read the file content
                    with open('MSB_CAMPAIGN.xlsx', 'rb') as file:
                        btn = st.download_button(
                            label="Download Excel",
                            data=file,
                            file_name="example_dataframe.xlsx",
                            mime="application/vnd.ms-excel"
                        )
       


   
if __name__ == "__run_main_app__":
    main()